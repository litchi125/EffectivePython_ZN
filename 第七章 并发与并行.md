# 第七章 并发与并行

​		并发性能够让计算机看起来同时多很多事情。例如，在一台只有一个CPU内核的计算机上，如果有哪个程序在单处理器上运行，操作系统就会迅速变化。这样做时，操作系统交错地执行程序，这就展示了一种多个程序同时进行的假象。

​		相反，并行性涉到在同一时间做许多不同的事情。具有多个CPU内核的计算机可以同时执行多个程序。每个CPU内核运行一个独立程序的指令，这允许每个程序在同一时刻前进。

​		在单个程序中，并发性是一种使程序员更容易解决某些类型问题的工具。并发程序支持许多不同的执行路径，包括独立的I/O流，以一种似乎同时和独立的方式向前推进。

​		并行性和并发性之间的关键区别在于加速。当一个程序中有两条不同的路径并行前进时，完成全部工作所需要的时间就减少了一半；执行速度快了两倍。相比之下，并发程序可能会运行数千条看似并行的独立执行路径，但对总体工作没有提供加速。

​		Python有多种风格方式能够使得并发程序变得很容易写。线程支持相对较少的并发性，而协程支持大量的并发函数。Python还可以通过系统调用、子进程和C扩展完成并行工作。但是让并发的Python代码真正并行运行是非常困难的。了解如何在这不同的情况下最好的利用Python是最重要的。

## 第52条 使用subprocess管理子进程

​		Python有运行和管理子进程的可靠库。这使得它成为其他工具(如命令行实用程序)结合在一起的一种很好的语言。当现有的shell脚本变得复杂时(随着时间的推移，他们经常会变得复杂)，出于可读性和可维护性的考虑，将他们分别使用Python重写是一个自然的选择。

​		由Python启动子进程能够并行运行，是我们能够使用Python消耗机器的所有CPU内核，并最大限度地提高程序的吞吐量。虽然Python本身可能受CPU限制(参见第53条)，但使用Python来驱动和协调CPU密集型工作的负载是很容易的。

​		Python有很多方式来运行子进程(例如，os.open，os.exex*)，但是管理子进程的最佳选择是使用子进程内置模块。用子进程运行子进程非常简单。这里，使用模块的run convenience函数来启动进程，读取其输出，并验证它是否终止彻底:

```python
import subprocess
# Enable these lines to make this example work on Windows
# import os
# os.environ['COMSPEC'] = 'powershell'

result = subprocess.run(
    ['echo', 'Hello from the child!'],
    capture_output=True,
    # Enable this line to make this example work on Windows
    # shell=True,
    encoding='utf-8', shell=True)

result.check_returncode()  # No exception means it exited cleanly
print(result.stdout)
>>>
"Hello from the child!"
```

***

**注意**

​		本项目中的示例假设系统存在echo、sleep、OpenSSL命令可用。在Windows上，情况并非如此。关于如何在Windows上运行这些代码片段，参阅完整示例代码。

***

​		子进程独立于父进程(Python解析器)而运行。如果使用popen类而不是run函数创建一个子进程，可以在Python做其他工作时轮询子进程。

~~~python
proc = subprocess.Popen(['sleep', '1'], shell=True)
while proc.poll() is None:
    print('Working...')
    # Some time-consuming work here
	...
print('Exit status', proc.poll())
>>>
Working...
Working...
Working...
Working...
Working...
Working...
...
~~~

​		将子进程与父进程解耦可释放父进程以并行地运行许多子进程。在这里，通过使用Popen提前启动所有的子进程来做到这一点：

~~~python
import time
start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell = True)
    sleep_procs.append(proc)
~~~

​		之后，等待他们完成I/O，用通信方法结束：

~~~python
import time

start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell=True)
    sleep_procs.append(proc)

for proc in sleep_procs:
    proc.communicate()
end = time.time()
delta = end - start
print(f'Finished in {delta:.3} seconds')
>>>
Finished in 0.111 seconds
~~~

​		如果这些进程按照顺序进行，总的延迟将会是10s或更多，而不是测试的结果。

​		还可以将数据从Python程序传输到子进程，并检查其输出。这允许使用许多其他程序并行运行工作。例如，假设要使用OpenSSL命令工具加密一些数据。使用命令行参数和I/O管道启动子进程很简单。

~~~python
import os
# On Windows, after installing OpenSSL, you may need to
# alias it in your PowerShell path with a command like:
# $env:path = $env:path + ";C:\Program Files\OpenSSL-Win64\bin"

def run_encrypt(data):
    env = os.environ.copy()
    env['password'] = 'zf7ShyBhZOraQDdE/FiZpm/m/8f9X+M1'
    proc = subprocess.Popen(
        ['openssl', 'enc', '-des3', '-pass', 'env:password'],
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
    	shell=True)
    proc.stdin.write(data)
    proc.stdin.flush()  # Ensure that the child gets input
    return proc

~~~

​		在这里，将随机字节通过管道传输到加密函数中，但实际上，这个输入管道将从用户输入、文件句柄、网络套接字等输入数据:

~~~python
procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_encrypt(data)
    procs.append(proc)
~~~

​		子进程并行运行并使用它们的输入。在这里，等待它们完成后，然后检索它们的最终输出。输出是随机的：

~~~python
for proc in procs:
    out, _ = proc.communicate()
    print(out[-10:])
>>>
b't\xcb|j\x8b\xf0\x96P\x85\xa0'
b'\xd5M\xd1.\xa5\xb4C\xb0\xd3_'
b'3\xc4]\x07YB\xd7X\xf1\xd8'

~~~

​		还可以创建并行进程链，就像UNIX管道一样，将一个子进程的输出连接到另一个子进程的输入，等等。下面是一个函数，它将openssl命令行工具作为子进程启动，以生成输入流的Whirlpool散列：

~~~python
def run_hash(input_stdin):
    return subprocess.Popen(
        ['openssl', 'dgst', '-whirlpool', '-binary'],
        stdin=input_stdin,
        stdout=subprocess.PIPE,
    	shell=True)
~~~

​		现在，可以启动一组进程来加密一些数据，然后启动另一组进程来对其加密的输出进行散列。这里必须注意，启动子进程管道的Python解释器进程是如何保留上游进程的stdout实例的:

~~~python
encrypt_procs = []
hash_procs = []
for _ in range(3):
    data = os.urandom(100)

    encrypt_proc = run_encrypt(data)
    encrypt_procs.append(encrypt_proc)

    hash_proc = run_hash(encrypt_proc.stdout)
    hash_procs.append(hash_proc)

    # Ensure that the child consumes the input stream and
    # the communicate() method doesn't inadvertently steal
    # input from the child. Also lets SIGPIPE propagate to
    # the upstream process if the downstream process dies.
    encrypt_proc.stdout.close()
    encrypt_proc.stdout = None
~~~

​		一旦启动子进程，它们之间的I/O将自动发生。所需要做的就是等待他们完成并打印最终输出:

```python
for proc in encrypt_procs:
    proc.communicate()
    assert proc.returncode == 0

for proc in hash_procs:
    out, _ = proc.communicate()
    print(out[-10:])
    assert proc.returncode == 0
>>>
b'\xe2j\x98h\xfd\xec\xe7T\xd84'
b'\xf3.i\x01\xd74|\xf2\x94E'
b'5_n\xc3-\xe6j\xeb[i'
```

​		如果担心子进程永远不会完成或以某种方式阻塞输入或输出管道，可以将timeout参数传递给communicate方法。如果子进程没有在指定的时间内完成，就会引发一个异常，从而可以终止行为不正常的子进程:

```python
# Use this line instead to make this example work on Windows
# proc = subprocess.Popen(['sleep', '10'], shell=True)
proc = subprocess.Popen(['sleep', '10'], shell=True)
try:
    proc.communicate(timeout=0.1)
except subprocess.TimeoutExpired:
    proc.terminate()
    proc.wait()

print('Exit status', proc.poll())
```

**要点**

* 使用子进程模块来运行子进程并管理它们的输入和输出流
* 子进程与Python解释器并行运行，使得能够最大限度地利用CPU内核
* run便利函数的简单使用，Popen类的高级使用，如unix风格的管道
* 使用通信方法的timeout参数来避免死锁和子进程挂起

## 第53条 使用线程阻塞I/O以避免并行

​		**Python的标准实现成为CPython。CPython以两步运行Python。首先，他将源文本解析并编译成字节码，这是作为8位指令的程序的低级表示。(然而，从Python3.6开始，他在技术上是带有16位指令的字节码，但其意思是相同的。)然后，CPython使用基于堆栈的解析器运行字节码。字节码解释器的状态必须在Python程序执行时保持一致。CPython通过一种称为全局解析器(GIL)的机制来加强一致性。**

​		本质上，GIL是一个互斥锁，它防止CPython受到抢占式多线程影响，抢占式多线程是指一个线程通过中断另一个线程来控制一个程序。这样的中断如果发生在意外的时间，可能会破坏解释器状态(例如，垃圾收集引用计数)。GIL防止了这些中断，并确保每个字节码指令与CPython实现及其C扩展模块正常工作。

​		GIL有一个重要的副作用。对于用c++或Java这样的语言编写的程序，拥有多个执行线程意味着一个程序可以同时使用多个CPU内核。尽管Python支持多个执行线程，但GIL每次只会让其中一个线程取得进展。这意味着当使用线程来进行并行计算并加速Python程序时，并不一定如你所愿。

​		例如，要使用Python进行一些计算密集型的工作。这里使用一个简单的数据分解操作代理执行：

~~~python
def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i
~~~

​		分解一组连续的数字需要相当长的时间：

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


numbers = [2139079, 1214759, 1516637, 1852285]
start = time.time()
for number in numbers:
    list(factorize(number))
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.555 seconds
```

​		使用多线程来完成这个计算在其他语言是有意义的，因为可以利用计算机的所有CPU内核。这里用Python试试。定义一个Python线程来执行与之前相同的计算：

```python
from threading import Thread
class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def run(self):
        self.factors = list(factorize(self.number))
```

​	然后，我启动一个线程来并行分解每个数字:

```python
start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
```

​	最后，我等待所有线程完成:

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


from threading import Thread


class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number

    def run(self):
        self.factors = list(factorize(self.number))


start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.480 seconds
```

​		令人惊讶的是，这比串行运行中运行factorize话费的时间并不少多少。对于每个数字一个线程，由于创建线程和协调线程的开销，可能并不会像其他语言那样提高四倍的速率。可能希望在用来运行此代码的双核机器上只有2倍的速度。但是，当需要使用多个cpu时，这些线程的性能不会更好。这演示了GIL(例如，锁争用和调度开销)对在标准CPython解释器中运行的程序的影响。

​		有些方法可以让CPython使用多个核从而大幅度提高运行效率，但他们不能与标准的Thread类一起工作(参见第64条)。既然有这些限制，为什么Python还要支持多线程呢？有两个很重要的原因。

​		第一、多线程让程序很容易在同一时间看上去并行工作。同时处理多项任务是很难自己做到的(参见第56条)。有了多线程，就可以让Python并发运行函数。这是可行的，因为CPython确保了Python执行的线程之间一定程度的公平性，即使GIL，每次只有一个线程取得进展。

​		第二、是为了处理I/O阻塞，当Python执行某些类型的系统操作时，阻塞I/O会发生。

​		Python程序使用系统调用来完成计算机的操作系统与外部环境进行交互。阻塞I/O包括读取和写入文件、与网络交互、与显示器等设备通信等。线程将程序与操作系统响应请求所花费的时间隔离开来，从而帮助处理阻塞I/O。

​		例如，假设要通过串口向远程控制的直升机发送一个信号。将使用一个缓慢的系统调用作为这个活动的代理。这个函数要求操作系统阻塞0.1秒，然后将控制放回给程序，这类似于同步串行端口发生的情况：

~~~python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)
~~~

​		串行运行这个系统调用需要线性增加的时间:

```python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)

start = time.time()
for _ in range(5):
    slow_systemcall()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.502 seconds
```

​		这样做的问题是，当slow_systemcall函数正在运行时，程序不能取得任何进展。程序执行的主线程被阻塞在select系统调用上。这种情况在实践中是非常可怕的。当向计算机发送信号时，需要能够计算出它的下一步行动；否则，它会崩溃。当发现需要同时执行阻塞I/O和计算时，就应该考虑将系统调用转移到线程中。

​		在这里，在单独的线程中运行多个slow_systemcall函数的调用。这将允许与多个串行端口(和直升机)在同一时间，同时离开主线程做任何需要的计算:

```python
start = time.time()
threads = []
for _ in range(5):
    thread = Thread(target=slow_systemcall)
thread.start()
threads.append(thread)
```

​		随着线程开始，这里做了一些工作来计算下一个直升机移动之前等待系统调用线程完成:

```
def compute_helicopter_location(index):
    ...
for i in range(5):
    compute_helicopter_location(i)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.108 seconds
```

​		**并行时间比串行时间小~5倍。这表明所有的系统调用都将从多个Python线程并行运行，即使它们受到GIL的限制。GIL阻止我的Python代码并行运行，但是它对系统调用没有影响。这是因为Python线程在进行系统调用之前释放GIL，并在系统调用完成后重新获取GIL。**

​		除了使用线程之外，还有许多其他方法来处理阻塞I/O，比如asyncio内置模块，这些替代方法有重要的好处。但是这些选项可能需要在重构代码以适应不同的执行模型时进行额外的工作(参见第60条和第62条)。使用线程是并行执行阻塞I/O的最简单方法，同时对程序进行最小的更改。

**要点**

* 由于全局解释器锁(GIL)， Python线程不能在多个CPU内核上并行运行
* 尽管有GIL, Python线程仍然是有用的，因为它们提供了一种简单的方法，似乎可以同时做多个事情
* 使用Python线程并行进行多个系统调用。这允许您在进行计算的同时进行阻塞I/O 

