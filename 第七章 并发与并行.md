# 第七章 并发与并行

​		并发性能够让计算机看起来同时多很多事情。例如，在一台只有一个CPU内核的计算机上，如果有哪个程序在单处理器上运行，操作系统就会迅速变化。这样做时，操作系统交错地执行程序，这就展示了一种多个程序同时进行的假象。

​		相反，并行性涉到在同一时间做许多不同的事情。具有多个CPU内核的计算机可以同时执行多个程序。每个CPU内核运行一个独立程序的指令，这允许每个程序在同一时刻前进。

​		在单个程序中，并发性是一种使程序员更容易解决某些类型问题的工具。并发程序支持许多不同的执行路径，包括独立的I/O流，以一种似乎同时和独立的方式向前推进。

​		并行性和并发性之间的关键区别在于加速。当一个程序中有两条不同的路径并行前进时，完成全部工作所需要的时间就减少了一半；执行速度快了两倍。相比之下，并发程序可能会运行数千条看似并行的独立执行路径，但对总体工作没有提供加速。

​		Python有多种风格方式能够使得并发程序变得很容易写。线程支持相对较少的并发性，而协程支持大量的并发函数。Python还可以通过系统调用、子进程和C扩展完成并行工作。但是让并发的Python代码真正并行运行是非常困难的。了解如何在这不同的情况下最好的利用Python是最重要的。

## 第52条 使用subprocess管理子进程

​		Python有运行和管理子进程的可靠库。这使得它成为其他工具(如命令行实用程序)结合在一起的一种很好的语言。当现有的shell脚本变得复杂时(随着时间的推移，他们经常会变得复杂)，出于可读性和可维护性的考虑，将他们分别使用Python重写是一个自然的选择。

​		由Python启动子进程能够并行运行，是我们能够使用Python消耗机器的所有CPU内核，并最大限度地提高程序的吞吐量。虽然Python本身可能受CPU限制(参见第53条)，但使用Python来驱动和协调CPU密集型工作的负载是很容易的。

​		Python有很多方式来运行子进程(例如，os.open，os.exex*)，但是管理子进程的最佳选择是使用子进程内置模块。用子进程运行子进程非常简单。这里，使用模块的run convenience函数来启动进程，读取其输出，并验证它是否终止彻底:

```python
import subprocess
# Enable these lines to make this example work on Windows
# import os
# os.environ['COMSPEC'] = 'powershell'

result = subprocess.run(
    ['echo', 'Hello from the child!'],
    capture_output=True,
    # Enable this line to make this example work on Windows
    # shell=True,
    encoding='utf-8', shell=True)

result.check_returncode()  # No exception means it exited cleanly
print(result.stdout)
>>>
"Hello from the child!"
```

***

**注意**

​		本项目中的示例假设系统存在echo、sleep、OpenSSL命令可用。在Windows上，情况并非如此。关于如何在Windows上运行这些代码片段，参阅完整示例代码。

***

​		子进程独立于父进程(Python解析器)而运行。如果使用popen类而不是run函数创建一个子进程，可以在Python做其他工作时轮询子进程。

~~~python
proc = subprocess.Popen(['sleep', '1'], shell=True)
while proc.poll() is None:
    print('Working...')
    # Some time-consuming work here
	...
print('Exit status', proc.poll())
>>>
Working...
Working...
Working...
Working...
Working...
Working...
...
~~~

​		将子进程与父进程解耦可释放父进程以并行地运行许多子进程。在这里，通过使用Popen提前启动所有的子进程来做到这一点：

~~~python
import time
start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell = True)
    sleep_procs.append(proc)
~~~

​		之后，等待他们完成I/O，用通信方法结束：

~~~python
import time

start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell=True)
    sleep_procs.append(proc)

for proc in sleep_procs:
    proc.communicate()
end = time.time()
delta = end - start
print(f'Finished in {delta:.3} seconds')
>>>
Finished in 0.111 seconds
~~~

​		如果这些进程按照顺序进行，总的延迟将会是10s或更多，而不是测试的结果。

​		还可以将数据从Python程序传输到子进程，并检查其输出。这允许使用许多其他程序并行运行工作。例如，假设要使用OpenSSL命令工具加密一些数据。使用命令行参数和I/O管道启动子进程很简单。

~~~python
import os
# On Windows, after installing OpenSSL, you may need to
# alias it in your PowerShell path with a command like:
# $env:path = $env:path + ";C:\Program Files\OpenSSL-Win64\bin"

def run_encrypt(data):
    env = os.environ.copy()
    env['password'] = 'zf7ShyBhZOraQDdE/FiZpm/m/8f9X+M1'
    proc = subprocess.Popen(
        ['openssl', 'enc', '-des3', '-pass', 'env:password'],
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
    	shell=True)
    proc.stdin.write(data)
    proc.stdin.flush()  # Ensure that the child gets input
    return proc

~~~

​		在这里，将随机字节通过管道传输到加密函数中，但实际上，这个输入管道将从用户输入、文件句柄、网络套接字等输入数据:

~~~python
procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_encrypt(data)
    procs.append(proc)
~~~

​		子进程并行运行并使用它们的输入。在这里，等待它们完成后，然后检索它们的最终输出。输出是随机的：

~~~python
for proc in procs:
    out, _ = proc.communicate()
    print(out[-10:])
>>>
b't\xcb|j\x8b\xf0\x96P\x85\xa0'
b'\xd5M\xd1.\xa5\xb4C\xb0\xd3_'
b'3\xc4]\x07YB\xd7X\xf1\xd8'

~~~

​		还可以创建并行进程链，就像UNIX管道一样，将一个子进程的输出连接到另一个子进程的输入，等等。下面是一个函数，它将openssl命令行工具作为子进程启动，以生成输入流的Whirlpool散列：

~~~python
def run_hash(input_stdin):
    return subprocess.Popen(
        ['openssl', 'dgst', '-whirlpool', '-binary'],
        stdin=input_stdin,
        stdout=subprocess.PIPE,
    	shell=True)
~~~

​		现在，可以启动一组进程来加密一些数据，然后启动另一组进程来对其加密的输出进行散列。这里必须注意，启动子进程管道的Python解释器进程是如何保留上游进程的stdout实例的:

~~~python
encrypt_procs = []
hash_procs = []
for _ in range(3):
    data = os.urandom(100)

    encrypt_proc = run_encrypt(data)
    encrypt_procs.append(encrypt_proc)

    hash_proc = run_hash(encrypt_proc.stdout)
    hash_procs.append(hash_proc)

    # Ensure that the child consumes the input stream and
    # the communicate() method doesn't inadvertently steal
    # input from the child. Also lets SIGPIPE propagate to
    # the upstream process if the downstream process dies.
    encrypt_proc.stdout.close()
    encrypt_proc.stdout = None
~~~

​		一旦启动子进程，它们之间的I/O将自动发生。所需要做的就是等待他们完成并打印最终输出:

```python
for proc in encrypt_procs:
    proc.communicate()
    assert proc.returncode == 0

for proc in hash_procs:
    out, _ = proc.communicate()
    print(out[-10:])
    assert proc.returncode == 0
>>>
b'\xe2j\x98h\xfd\xec\xe7T\xd84'
b'\xf3.i\x01\xd74|\xf2\x94E'
b'5_n\xc3-\xe6j\xeb[i'
```

​		如果担心子进程永远不会完成或以某种方式阻塞输入或输出管道，可以将timeout参数传递给communicate方法。如果子进程没有在指定的时间内完成，就会引发一个异常，从而可以终止行为不正常的子进程:

```python
# Use this line instead to make this example work on Windows
# proc = subprocess.Popen(['sleep', '10'], shell=True)
proc = subprocess.Popen(['sleep', '10'], shell=True)
try:
    proc.communicate(timeout=0.1)
except subprocess.TimeoutExpired:
    proc.terminate()
    proc.wait()

print('Exit status', proc.poll())
```

**要点**

* 使用子进程模块来运行子进程并管理它们的输入和输出流
* 子进程与Python解释器并行运行，使得能够最大限度地利用CPU内核
* run便利函数的简单使用，Popen类的高级使用，如unix风格的管道
* 使用通信方法的timeout参数来避免死锁和子进程挂起

## 第53条 使用线程阻塞I/O以避免并行

​		**Python的标准实现成为CPython。CPython以两步运行Python。首先，他将源文本解析并编译成字节码，这是作为8位指令的程序的低级表示。(然而，从Python3.6开始，他在技术上是带有16位指令的字节码，但其意思是相同的。)然后，CPython使用基于堆栈的解析器运行字节码。字节码解释器的状态必须在Python程序执行时保持一致。CPython通过一种称为全局解析器(GIL)的机制来加强一致性。**

​		本质上，GIL是一个互斥锁，它防止CPython受到抢占式多线程影响，抢占式多线程是指一个线程通过中断另一个线程来控制一个程序。这样的中断如果发生在意外的时间，可能会破坏解释器状态(例如，垃圾收集引用计数)。GIL防止了这些中断，并确保每个字节码指令与CPython实现及其C扩展模块正常工作。

​		GIL有一个重要的副作用。对于用c++或Java这样的语言编写的程序，拥有多个执行线程意味着一个程序可以同时使用多个CPU内核。尽管Python支持多个执行线程，但GIL每次只会让其中一个线程取得进展。这意味着当使用线程来进行并行计算并加速Python程序时，并不一定如你所愿。

​		例如，要使用Python进行一些计算密集型的工作。这里使用一个简单的数据分解操作代理执行：

~~~python
def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i
~~~

​		分解一组连续的数字需要相当长的时间：

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


numbers = [2139079, 1214759, 1516637, 1852285]
start = time.time()
for number in numbers:
    list(factorize(number))
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.555 seconds
```

​		使用多线程来完成这个计算在其他语言是有意义的，因为可以利用计算机的所有CPU内核。这里用Python试试。定义一个Python线程来执行与之前相同的计算：

```python
from threading import Thread
class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def run(self):
        self.factors = list(factorize(self.number))
```

​	然后，我启动一个线程来并行分解每个数字:

```python
start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
```

​	最后，我等待所有线程完成:

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


from threading import Thread


class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number

    def run(self):
        self.factors = list(factorize(self.number))


start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.480 seconds
```

​		令人惊讶的是，这比串行运行中运行factorize话费的时间并不少多少。对于每个数字一个线程，由于创建线程和协调线程的开销，可能并不会像其他语言那样提高四倍的速率。可能希望在用来运行此代码的双核机器上只有2倍的速度。但是，当需要使用多个cpu时，这些线程的性能不会更好。这演示了GIL(例如，锁争用和调度开销)对在标准CPython解释器中运行的程序的影响。

​		有些方法可以让CPython使用多个核从而大幅度提高运行效率，但他们不能与标准的Thread类一起工作(参见第64条)。既然有这些限制，为什么Python还要支持多线程呢？有两个很重要的原因。

​		第一、多线程让程序很容易在同一时间看上去并行工作。同时处理多项任务是很难自己做到的(参见第56条)。有了多线程，就可以让Python并发运行函数。这是可行的，因为CPython确保了Python执行的线程之间一定程度的公平性，即使GIL，每次只有一个线程取得进展。

​		第二、是为了处理I/O阻塞，当Python执行某些类型的系统操作时，阻塞I/O会发生。

​		Python程序使用系统调用来完成计算机的操作系统与外部环境进行交互。阻塞I/O包括读取和写入文件、与网络交互、与显示器等设备通信等。线程将程序与操作系统响应请求所花费的时间隔离开来，从而帮助处理阻塞I/O。

​		例如，假设要通过串口向远程控制的直升机发送一个信号。将使用一个缓慢的系统调用作为这个活动的代理。这个函数要求操作系统阻塞0.1秒，然后将控制放回给程序，这类似于同步串行端口发生的情况：

~~~python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)
~~~

​		串行运行这个系统调用需要线性增加的时间:

```python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)

start = time.time()
for _ in range(5):
    slow_systemcall()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.502 seconds
```

​		这样做的问题是，当slow_systemcall函数正在运行时，程序不能取得任何进展。程序执行的主线程被阻塞在select系统调用上。这种情况在实践中是非常可怕的。当向计算机发送信号时，需要能够计算出它的下一步行动；否则，它会崩溃。当发现需要同时执行阻塞I/O和计算时，就应该考虑将系统调用转移到线程中。

​		在这里，在单独的线程中运行多个slow_systemcall函数的调用。这将允许与多个串行端口(和直升机)在同一时间，同时离开主线程做任何需要的计算:

```python
start = time.time()
threads = []
for _ in range(5):
    thread = Thread(target=slow_systemcall)
thread.start()
threads.append(thread)
```

​		随着线程开始，这里做了一些工作来计算下一个直升机移动之前等待系统调用线程完成:

```
def compute_helicopter_location(index):
    ...
for i in range(5):
    compute_helicopter_location(i)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.108 seconds
```

​		**并行时间比串行时间小~5倍。这表明所有的系统调用都将从多个Python线程并行运行，即使它们受到GIL的限制。GIL阻止我的Python代码并行运行，但是它对系统调用没有影响。这是因为Python线程在进行系统调用之前释放GIL，并在系统调用完成后重新获取GIL。**

​		除了使用线程之外，还有许多其他方法来处理阻塞I/O，比如asyncio内置模块，这些替代方法有重要的好处。但是这些选项可能需要在重构代码以适应不同的执行模型时进行额外的工作(参见第60条和第62条)。使用线程是并行执行阻塞I/O的最简单方法，同时对程序进行最小的更改。

**要点**

* 由于全局解释器锁(GIL)， Python线程不能在多个CPU内核上并行运行
* 尽管有GIL, Python线程仍然是有用的，因为它们提供了一种简单的方法，似乎可以同时做多个事情
* 使用Python线程并行进行多个系统调用。这允许您在进行计算的同时进行阻塞I/O 

## 第54条 使用lock防止线程中的数据竞争

​		在学习了全局解释器锁(GIL)之后(参见第53条)，许多新的Python程序员认为他们可以完全放弃在他们的代码中使用互斥锁(也称为互斥锁)。如果GIL已经在阻止Python线程在多个CPU内核上并行运行，那么它还必须充当程序数据结构的锁，对吗?对列表和字典等类型的一些测试甚至可能显示这一假设似乎成立。

​		但要注意，事实并非如此。GIL不会保护你的。尽管一次只有一个Python线程运行，但线程对数据结构的操作可以在Python解释器中的任何两个字节码指令之间中断。如果同时从多个线程访问相同的对象，这是危险的。由于这些中断，数据结构的不变量实际上随时都可能被违反，使程序处于损坏状态。

​		例如，假设想编写一个并行计算许多事情的程序，比如从整个传感器网络中采样光级。如果想要确定一段时间内轻样本的总数，可以用一个新类来聚合它们：

​		假设每个传感器都有自己的工作线程，因为从传感器读取数据需要阻塞I/O。在每个传感器测量之后，工作线程将计数器增加到期望的最大读数:

​		在这里，为每个传感器并行运行一个工作线程，并等待它们完成它们的读数:

```python
class Counter:
    def __init__(self):
        self.count = 0

    def increment(self, offset):
        self.count += offset

def worker(sensor_index, how_many, counter):
    for _ in range(how_many):
        # Read from the sensor
        ...
        counter.increment(1)

from threading import Thread
how_many = 10**5
counter = Counter()
threads = []
for i in range(5):
    thread = Thread(target=worker,
                    args=(i, how_many, counter))
    threads.append(thread)
    thread.start()
for thread in threads:
    thread.join()
expected = how_many * 5
found = counter.count
print(f'Counter should be {expected}, got {found}')
>>>
Counter should be 500000, got 415471
```

​		这看起来很简单，结果也应该很明显，但结果却大相径庭!这里发生了什么?这么简单的事情怎么会出这么大的问题，尤其是在一次只能运行一个Python解释器线程的情况下?

​		Python解释器强制执行所有线程之间的公平性，以确保它们得到大致相同的处理时间。为此，Python在线程运行时暂停一个线程，并依次恢复另一个线程。问题是不知道Python什么时候会挂起线程。线程甚至可以在看似原子操作的过程中中途暂停。这就是本案的情况。

​		Counter对象的increment方法体看起来很简单，从工作线程的角度来看，相当于下面的语句:

~~~python
counter.count += 1
~~~

​		但是对象属性上使用的+=操作符实际上指示Python在后台执行三个独立的操作。上面的语句相当于:

~~~python
value = getattr(counter, 'count')
result = value + 1
setattr(counter, 'count', result)
~~~

​		递增计数器的Python线程可以挂起在上述任意两个操作之间。如果操作的交错方式导致将旧版本的值赋给计数器，那么这就有问题了。以下是两个线程(A和B)之间糟糕交互的例子:

```python
# Running in Thread A
value_a = getattr(counter, 'count')
# Context switch to Thread B
value_b = getattr(counter, 'count')
result_b = value_b + 1
setattr(counter, 'count', result_b)
# Context switch back to Thread A
result_a = value_a + 1
setattr(counter, 'count', result_a)
```

​		线程B在线程A完全完成之前中断了它。线程B运行并结束，但是线程A在执行过程中恢复，覆盖了线程B的计数器增量的所有进程。这正是上面光传感器的例子中所发生的。

​		为了防止类似的数据竞争和其他形式的数据结构损坏，Python在线程内置模块中包含了一组健壮的工具。其中最简单也是最有用的是Lock类，这是一个互斥锁(互斥锁)。

​		通过使用锁，可以让Counter类保护其当前值不受多个线程的同时访问。每次只有一个线程能够获得锁。在这里，使用一个with语句来获取和释放锁; 这使得当锁被持有时，更容易看到哪个代码正在执行(参见第66条):

```python
from threading import Lock
class LockingCounter:
    def __init__(self):
        self.lock = Lock()
        self.count = 0
    def increment(self, offset):
        with self.lock:
            self.count += offset
```

​		现在，像以前一样运行工作线程，但是使用一个LockingCounter

```python
from threading import Lock
class LockingCounter:
    def __init__(self):
        self.lock = Lock()
        self.count = 0
    def increment(self, offset):
        with self.lock:
            self.count += offset

counter = LockingCounter()
for i in range(5):
    thread = Thread(target=worker,
                    args=(i, how_many, counter))
    threads.append(thread)
    thread.start()
for thread in threads:
    thread.join()
expected = how_many * 5
found = counter.count
print(f'Counter should be {expected}, got {found}')
>>>
Counter should be 500000, got 500000
```

​		结果正是我所期望的。lock解决了这个问题。

**要点**

* 即使Python有一个全局解释器锁，仍然要防止程序中线程之间的数据竞争
* 如果允许多个线程在没有互斥锁(互斥锁)的情况下修改相同的对象，程序将破坏它们的数据结构
* 使用threading内置模块中的Lock类在多个线程之间强制执行程序的不变

## 第55条 使用队列来协调线程之间的工作

​		同时做很多事情的Python程序通常需要协调它们的工作。对并发工作最有用的安排之一是功能管道。

​		管道的工作原理就像制造业中的装配线。管道有许多串行的阶段，每个阶段都有特定的功能。新的工作不断地被添加到管道的开始。这些函数可以并发操作，每个函数处理其阶段中的工作片段。随着每个功能的完成，工作将继续进行，直到没有剩下的阶段。这种方法特别适用于包含阻塞I/O或子进程活动的工作，这些活动可以很容易地使用Python并行化(参见第53条)。

​		例如，假设想建立一个系统，它将从我的数码相机中持续获取图像流，调整它们的大小，然后将它们添加到在线图片库中。这样一个程序可以分为一个管道的三个阶段。在第一阶段检索新的图像。下载的图像在第二阶段通过resize函数传递。调整大小后的图像将由上传功能在最后阶段使用。

​		假设已经编写了执行以下阶段的Python函数:下载、调整大小、上传。我如何组装一个管道来并发地完成工作?

~~~python
def download(item):
	...
def resize(item):
	...
def upload(item):
	...

~~~

​		需要的第一件事是在管道阶段之间传递工作的方法。可以将其建模为线程安全的生产者-消费者队列(参见第53条和第71条):

~~~python
from collections import deque
from threading import Lock


class MyQueue:
    def __init__(self):
        self.items = deque()
        self.lock = Lock()
~~~

​		生产者人也就是数码相机，把新图像添加到一堆待定物品的末尾:

```python
def put(self, item):
    with self.lock:
        self.items.append(item)
```

​		消费者是处理管道的第一阶段，它从挂起项的队列前面删除图像:

```python
def get(self):
    with self.lock:
        return self.items.popleft()
```

​		在这里，将管道的每个阶段表示为一个Python线程，该线程从一个队列中获取工作，在其中运行一个函数，并将结果放到另一个队列中。我还会跟踪工人检查新输入的次数以及它完成了多少工作:

```python
class Worker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0
```

​		最棘手的部分是，工作线程必须正确处理输入队列为空的情况，因为前一个阶段还没有完成它的工作。这发生在捕获下面的IndexError异常的地方。可以把这看作是流水线上的一个阻碍:

```python
class Worker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0

    def run(self):
        while True:
            self.polled_count += 1
            try:
                item = self.in_queue.get()
            except IndexError:
                time.sleep(0.01)  # No work to do
            else:
                result = self.func(item)
                self.out_queue.put(result)
                self.work_done += 1
```

​		现在，可以通过为它们的协调点和相应的工作线程创建队列来将这三个阶段连接在一起:

```python
download_queue = MyQueue()
resize_queue = MyQueue()
upload_queue = MyQueue()
done_queue = MyQueue()
threads = [
    Worker(download, download_queue, resize_queue),
    Worker(resize, resize_queue, upload_queue),
    Worker(upload, upload_queue, done_queue),
        ]
```

​		可以启动线程，然后将一堆工作注入管道的第一阶段。在这里，使用一个普通对象实例作为下载函数所需的真实数据的代理:

```python
for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
```

​		现在，等待所有的项目被管道处理，并结束在done_queue:

```python
while len(done_queue.items) < 1000:
    # Do something useful while waiting
    ...
```

​		这可以正常运行，但是有一个有趣的副作用，即线程轮询它们的输入队列以获取新工作。棘手的部分是，在run方法中捕获IndexError异常，执行了很多次:

```python
import time
from functools import wraps

from collections import deque
from threading import Lock
from threading import Thread


def download(item):
    ...


def resize(item):
    ...


def upload(item):
    ...


class MyQueue:
    def __init__(self):
        self.items = deque()
        self.lock = Lock()

    def put(self, item):
        with self.lock:
            self.items.append(item)

    def get(self):
        with self.lock:
            return self.items.popleft()


class Worker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0

    def run(self):
        while True:
            self.polled_count += 1
            try:
                item = self.in_queue.get()
            except IndexError:
                time.sleep(0.01)  # No work to do
            else:
                result = self.func(item)
                self.out_queue.put(result)
                self.work_done += 1


download_queue = MyQueue()
resize_queue = MyQueue()
upload_queue = MyQueue()
done_queue = MyQueue()
threads = [
    Worker(download, download_queue, resize_queue),
    Worker(resize, resize_queue, upload_queue),
    Worker(upload, upload_queue, done_queue),
]

for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
while len(done_queue.items) < 1000:
    # Do something useful while waiting
    ...

processed = len(done_queue.items)
polled = sum(t.polled_count for t in threads)
print(f'Processed {processed} items after '
      f'polling {polled} times')
>>>
Processed 1000 items after polling 3012 times
```

​		当worker函数以各自的速度变化时，早期阶段可以阻止后期阶段的进展，从而备份管道。这将导致后面的阶段挨饿，并在一个紧密循环中不断检查它们的输入队列以获取新工作。结果是，工作线程浪费了CPU时间，不做任何有用的事情;他们不断地引发和捕获IndexError异常。

​		但这只是这个实现问题的开始。还有三个问题也应该避免。首先，确定所有的输入工作都已完成，需要在done_queue上再进行一次繁忙的等待。其次，在Worker中，run方法将在其繁忙循环中永远执行。没有明显的方法来通知工作线程是时候退出了。

​		第三，也是最糟糕的是，管道中的备份可能会任意地导致程序崩溃。如果第一阶段进展较快，而第二阶段进展缓慢，则第一阶段与第二阶段连接的队列规模将不断增大。第二阶段将无法跟上。如果有足够的时间和输入数据，程序最终会耗尽内存并死亡。

​		这里的教训不是管道是糟糕的;而是很难自己构建一个好的生产者-消费者队列。所以为什么还要尝试呢?

**排队救援**

​		Queue内置模块中的Queue类提供了解决上述问题所需的所有功能。

​		Queue通过使get方法块直到新数据可用来消除工作者中的忙碌等待。例如，这里我启动了一个线程，在队列上等待一些输入数据:

```python
from queue import Queue
my_queue = Queue()
def consumer():
    print('Consumer waiting')
	my_queue.get() # Runs after put() below
print('Consumer done')
thread = Thread(target=consumer)
thread.start()
```

​		即使线程先运行，它也不会结束，直到一个条目被放到Queue实例中，并且get方法有东西要返回:

```python
from queue import Queue
my_queue = Queue()
def consumer():
    print('Consumer waiting')
	my_queue.get() # Runs after put() below
print('Consumer done')
thread = Thread(target=consumer)
thread.start()
print('Producer putting')
my_queue.put(object()) # Runs before get() above
print('Producer done')
thread.join()
>>>
Consumer done
Consumer waitingProducer putting
Producer done
```

​		为了解决管道备份问题，Queue类允许指定两个阶段之间允许的最大暂挂工作量。

​		这个缓冲区大小导致在队列已经满时调用put来阻塞。例如，这里我定义了一个线程，在消费一个队列之前等待一段时间:

```python
def consumer():
    time.sleep(0.1)  # Wait
    my_queue.get()  # Runs second
    print('Consumer got 1')
    my_queue.get()  # Runs fourth
    print('Consumer got 2')
    print('Consumer done')


thread = Thread(target=consumer)
thread.start()
```

​		等待应该允许生产者线程在消费者线程调用get之前将两个对象放到队列中。但是队列大小是其中之一。这意味着在第二次put调用停止阻塞并将第二项添加到队列中之前，生产者将不得不等待消费线程调用get至少一次:

```python
my_queue = Queue(1)  # Buffer size of 1


def consumer():
    time.sleep(0.1)  # Wait
    my_queue.get()  # Runs second
    print('Consumer got 1')
    my_queue.get()  # Runs fourth
    print('Consumer got 2')
    print('Consumer done')


thread = Thread(target=consumer)
thread.start()
my_queue.put(object()) # Runs first
print('Producer put 1')
my_queue.put(object()) # Runs third
print('Producer put 2')
print('Producer done')
thread.join()
>>>
Producer put 1
Consumer got 1
Producer put 2
Producer done
Consumer got 2
Consumer done
```

​		Queue类还可以使用task_done方法跟踪工作进度。这允许等待阶段的输入队列耗尽，并消除了轮询管道的最后一个阶段的需要(与上面的done_queue一样)。例如，这里定义了一个消费者线程，当它完成一个项目的工作时调用task_done:

```python
in_queue = Queue()
def consumer():
    print('Consumer waiting')
    work = in_queue.get() # Runs second
    print('Consumer working')
    # Doing work
    ...
    print('Consumer done')
    in_queue.task_done()  # Runs third

thread = Thread(target=consumer)
thread.start()
```

​		现在，生产者代码不必加入消费者线程或轮询。生产者可以通过调用Queue实例上的join来等待in_queue完成。即使它是空的，in_queue也不会是可连接的，直到对所有已排队的项调用task_done之后:

```python
in_queue = Queue()


def consumer():
    print('Consumer waiting')
    work = in_queue.get()  # Runs second
    print('Consumer working')
    # Doing work
    ...
    print('Consumer done')
    in_queue.task_done()  # Runs third


thread = Thread(target=consumer)
thread.start()
print('Producer putting')
in_queue.put(object())  # Runs first
print('Producer waiting')
in_queue.join()  # Runs fourth
print('Producer done')
thread.join()
>>>
Consumer waiting
Producer putting
Producer waiting
Consumer working
Consumer done
Producer done
```

​		可以把所有这些行为放到一个Queue子类中，这个子类还会告诉工作线程它应该在什么时候停止处理。在这里，定义了一个close方法，它向队列中添加了一个特殊的哨兵项，表明在它之后将没有更多的输入项:

​		然后，为队列定义一个迭代器，用于查找这个特殊对象，并在找到它时停止迭代。这个--iter--方法也会在适当的时候调用task_done，来跟踪队列上的工作进度

```python
class ClosableQueue(Queue):
    SENTINEL = object()
    def close(self):
        self.put(self.SENTINEL)

    def __iter__(self):
        while True:
            item = self.get()
            try:
                if item is self.SENTINEL:
                    return  # Cause the thread to exit
                yield item
            finally:
                self.task_done()
```

​		现在，可以重新定义工作线程，以依赖于ClosableQueue类的行为。当for循环耗尽时，线程将退出:

```python
class StoppableWorker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
    def run(self):
        for item in self.in_queue:
            result = self.func(item)
            self.out_queue.put(result)
```

​		使用新的工作线程类重新创建了一组工作线程:

```python

class ClosableQueue(Queue):
    SENTINEL = object()

    def close(self):
        self.put(self.SENTINEL)

    def __iter__(self):
        while True:
            item = self.get()
            try:
                if item is self.SENTINEL:
                    return  # Cause the thread to exit
                yield item
            finally:
                self.task_done()


class StoppableWorker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue

    def run(self):
        for item in self.in_queue:
            result = self.func(item)
            self.out_queue.put(result)


download_queue = ClosableQueue()
resize_queue = ClosableQueue()
upload_queue = ClosableQueue()
done_queue = ClosableQueue()
threads = [
    StoppableWorker(download, download_queue, resize_queue),
    StoppableWorker(resize, resize_queue, upload_queue),
    StoppableWorker(upload, upload_queue, done_queue),
]
```

​		在像之前一样运行工作线程之后，也通过关闭第一阶段的输入队列来发送停止信号:

```python
for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
download_queue.close()
```

​		最后，通过加入连接各个阶段的队列来等待工作完成。每当一个阶段完成时，通过关闭它的输入队列来通知下一个阶段停止。最后，done_queue包含了所有的输出对象，如预期的那样:

```python
def download(item):
    ...


def resize(item):
    ...


def upload(item):
    ...

class ClosableQueue(Queue):
    SENTINEL = object()

    def close(self):
        self.put(self.SENTINEL)

    def __iter__(self):
        while True:
            item = self.get()
            try:
                if item is self.SENTINEL:
                    return  # Cause the thread to exit
                yield item
            finally:
                self.task_done()


class StoppableWorker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue

    def run(self):
        for item in self.in_queue:
            result = self.func(item)
            self.out_queue.put(result)


download_queue = ClosableQueue()
resize_queue = ClosableQueue()
upload_queue = ClosableQueue()
done_queue = ClosableQueue()
threads = [
    StoppableWorker(download, download_queue, resize_queue),
    StoppableWorker(resize, resize_queue, upload_queue),
    StoppableWorker(upload, upload_queue, done_queue),
]
for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
download_queue.close()
download_queue.join()
resize_queue.close()
resize_queue.join()
upload_queue.close()
upload_queue.join()
print(done_queue.qsize(), 'items finished')
for thread in threads:
    thread.join()
>>>
1000 items finished
```

​		这种方法可以扩展为在每个阶段使用多个工作线程，这可以增加I/O并行性，并显著提高这种类型的程序的速度。为此，首先定义一些启动和停止多个线程的辅助函数。stop_threads的工作方式是在每个使用线程的输入队列上调用一次close，这确保了所有的worker干净地退出:

```python
def start_threads(count, *args):
    threads = [StoppableWorker(*args) for _ in range(count)]
    for thread in threads:
        thread.start()
    return threads


def stop_threads(closable_queue, threads):
    for _ in threads:
        closable_queue.close()
    closable_queue.join()
    for thread in threads:
        thread.join()
```

​		然后，像之前一样将这些片段连接在一起，将要处理的对象放在管道的顶部，在此过程中加入队列和线程，最后使用结果:

```python
def start_threads(count, *args):
    threads = [StoppableWorker(*args) for _ in range(count)]
    for thread in threads:
        thread.start()
    return threads


def stop_threads(closable_queue, threads):
    for _ in threads:
        closable_queue.close()
    closable_queue.join()
    for thread in threads:
        thread.join()


download_queue = ClosableQueue()
resize_queue = ClosableQueue()
upload_queue = ClosableQueue()
done_queue = ClosableQueue()
download_threads = start_threads(
    3, download, download_queue, resize_queue)
resize_threads = start_threads(
    4, resize, resize_queue, upload_queue)
upload_threads = start_threads(
    5, upload, upload_queue, done_queue)
for _ in range(1000):
    download_queue.put(object())
stop_threads(download_queue, download_threads)
stop_threads(resize_queue, resize_threads)
stop_threads(upload_queue, upload_threads)
print(done_queue.qsize(), 'items finished')
>>>
1000 items finished
```

​		尽管Queue在这种线性管道的情况下工作得很好，但是还有许多其他情况需要考虑更好的工具(参见第60条)。

**要点**

* 管道是组织使用多个Python线程并发运行的工作序列(特别是I/O绑定程序)的好方法
* 在构建并发管道时要注意许多问题:忙则等待，如何告诉工人停止，以及潜在的内存爆炸
* Queue类具有构建健壮管道所需的所有工具:阻塞操作、缓冲区大小和join

## 第56条 知道如何识别何时并发是必要的

​		不可避免的是，随着程序范围的扩大，它也会变得更加复杂。以保持清晰性、可测试性和效率的方式处理扩展需求是编程中最困难的部分之一。也许最难处理的变化类型是从单线程程序转移到需要多行并发执行的程序。

​		让我用一个例子来说明可能遇到的这个问题。假设想实现Conway的Game of Life，这是有限状态自动机的一个经典例子。游戏规则很简单:有一个任意大小的二维网格。网格中的每个单元格要么是活的，要么是空的:

> ALIVE = '*'
> EMPTY = '-'

​		游戏每次只进行一次。每只扁虱，每个细胞都要计算相邻的8个细胞中还有多少存活。根据邻近细胞的数量，一个细胞决定它是继续生存、死亡还是再生。(我将在下文解释具体规则。)这是一个5 × 5 Game of Life网格的例子，四代后时间向右移动:

| 0     | 1     | 2     | 3     | 4     |
| ----- | ----- | ----- | ----- | ----- |
| -*--- | --*-- | --**- | --*-- | ----- |
| --**- | --**- | -*--- | -*--- | -**-- |
| ---*- | --**- | --**- | --*-- | ----- |
| ----- | ----- | ----- | ----- | ----- |

​		可以用一个简单的容器类表示每个单元格的状态。类必须有方法，允许获得和设置任何坐标的值。越界的坐标应该环绕，使网格像一个无限循环空间:

```python
ALIVE = '*'
EMPTY = '-'


class Grid:
    def __init__(self, height, width):
        self.height = height
        self.width = width
        self.rows = []
        for _ in range(self.height):
            self.rows.append([EMPTY] * self.width)

    def get(self, y, x):
        return self.rows[y % self.height][x % self.width]

    def set(self, y, x, state):
        self.rows[y % self.height][x % self.width] = state

    def __str__(self):
        ...
```

​		为了查看这个类的实际操作，可以创建一个Grid实例，并将其初始状态设置为一个称为滑翔机的经典形状:

```python
ALIVE = '*'
EMPTY = '-'


class Grid:
    def __init__(self, height, width):
        self.height = height
        self.width = width
        self.rows = []
        for _ in range(self.height):
            self.rows.append([EMPTY] * self.width)

    def get(self, y, x):
        return self.rows[y % self.height][x % self.width]

    def set(self, y, x, state):
        self.rows[y % self.height][x % self.width] = state

    def __str__(self):
        output = ''
        for row in self.rows:
            for cell in row:
                output += cell
            output += '\n'
        return output


grid = Grid(5, 9)
grid.set(0, 3, ALIVE)
grid.set(1, 4, ALIVE)
grid.set(2, 2, ALIVE)
grid.set(2, 3, ALIVE)
grid.set(2, 4, ALIVE)
print(grid)
>>>
---*-----
----*----
--***----
---------
---------
```

​		现在，需要一种方法来检索邻近细胞的状态。可以使用一个辅助函数来实现这一点，它查询网格并返回居住邻居的计数。为了减少耦合，使用一个简单的函数作为get参数，而不是传入整个Grid实例(参见第38条):

```python
def count_neighbors(y, x, get):
    n_ = get(y - 1, x + 0) # North
    ne = get(y - 1, x + 1) # Northeast
    e_ = get(y + 0, x + 1) # East
    se = get(y + 1, x + 1) # Southeast
    s_ = get(y + 1, x + 0) # South
    sw = get(y + 1, x - 1) # Southwest
    w_ = get(y + 0, x - 1) # West
    nw = get(y - 1, x - 1) # Northwest
    neighbor_states = [n_, ne, e_, se, s_, sw, w_, nw]
    count = 0
    for state in neighbor_states:
        if state == ALIVE:
            count += 1
    return count
alive = {(9, 5), (9, 6)}
seen = set()
def fake_get(y, x):
    position = (y, x)
    seen.add(position)
    return ALIVE if position in alive else EMPTY

count = count_neighbors(10, 5, fake_get)
expected_seen = {
    (9, 5),  (9, 6),  (10, 6), (11, 6),
    (11, 5), (11, 4), (10, 4), (9, 4)
}
print(seen)
>>>
{(10, 4), (9, 6), (11, 6), (9, 5), (10, 6), (11, 5), (11, 4), (9, 4)}
```

​		现在，根据游戏的三个规则来定义Conway的生命游戏的简单逻辑:如果一个细胞的邻居少于两个就死亡，如果一个细胞的邻居多于三个就死亡，如果一个空的细胞恰好有三个邻居就活:

```python
def game_logic(state, neighbors):
    if state == ALIVE:
        if neighbors < 2:
            return EMPTY  # Die: Too few
        elif neighbors > 3:
            return EMPTY  # Die: Too many
    else:
        if neighbors == 3:
            return ALIVE  # Regenerate
    return state
```

​		可以在另一个转换单元格状态的函数中连接count_neighbors和game_logic。这个函数将在每一代中被调用，以找出一个单元格的当前状态，检查它周围的相邻单元格，确定它的下一个状态应该是什么，并相应地更新结果网格。同样，使用一个函数接口来设置，而不是传入Grid实例，以使代码更解耦:

```python
def step_cell(y, x, get, set):
    state = get(y, x)
    neighbors = count_neighbors(y, x, get)
    next_state = game_logic(state, neighbors)
    set(y, x, next_state)
    
alive = {(10, 5), (9, 5), (9, 6)}
new_state = None
def fake_get(y, x):
    return ALIVE if (y, x) in alive else EMPTY

def fake_set(y, x, state):
    global new_state
    new_state = state

# Stay alive
step_cell(10, 5, fake_get, fake_set)
print(new_state)
# Stay dead
alive.remove((10, 5))
step_cell(10, 5, fake_get, fake_set)
print(new_state)
# Regenerate
alive.add((10, 6))
step_cell(10, 5, fake_get, fake_set)
print(new_state)
>>>
*
-
*
```

​		最后，可以定义一个函数，将整个网格的单元格向前推进一步，然后返回一个包含下一代状态的新网格。这里重要的细节是，需要所有依赖函数来调用上一代Grid实例上的get方法，并调用下一代Grid实例上的set方法。这是确保所有单元格都能同步移动的方法，这也是游戏运作的重要部分。这很容易实现，因为使用了get和set函数接口，而不是传递Grid实例:

```python
def simulate(grid):
    next_grid = Grid(grid.height, grid.width)
    for y in range(grid.height):
        for x in range(grid.width):
            step_cell(y, x, grid.get, next_grid.set)
    return next_grid
```

​		现在，可以让grid每一代向前发展。根据game_logic函数的简单规则，可以看到滑翔机是如何在网格上向下和向右移动的:

```python
def simulate(grid):
    next_grid = Grid(grid.height, grid.width)
    for y in range(grid.height):
        for x in range(grid.width):
            step_cell(y, x, grid.get, next_grid.set)
    return next_grid


class ColumnPrinter:
    def __init__(self):
        self.columns = []

    def append(self, data):
        self.columns.append(data)

    def __str__(self):
        row_count = 1
        for data in self.columns:
            row_count = max(
                row_count, len(data.splitlines()) + 1)

        rows = [''] * row_count
        for j in range(row_count):
            for i, data in enumerate(self.columns):
                line = data.splitlines()[max(0, j - 1)]
                if j == 0:
                    padding = ' ' * (len(line) // 2)
                    rows[j] += padding + str(i) + padding
                else:
                    rows[j] += line

                if (i + 1) < len(self.columns):
                    rows[j] += ' | '

        return '\n'.join(rows)

grid = Grid(5, 9)
grid.set(0, 3, ALIVE)
grid.set(1, 4, ALIVE)
grid.set(2, 2, ALIVE)
grid.set(2, 3, ALIVE)
grid.set(2, 4, ALIVE)

columns = ColumnPrinter()
for i in range(5):
    columns.append(str(grid))
grid = simulate(grid)
print(columns)
>>>
    0     |     1     |     2     |     3     |     4    
---*----- | ---*----- | ---*----- | ---*----- | ---*-----
----*---- | ----*---- | ----*---- | ----*---- | ----*----
--***---- | --***---- | --***---- | --***---- | --***----
--------- | --------- | --------- | --------- | ---------
--------- | --------- | --------- | --------- | ---------
```

​		对于可以在一台机器上的一个线程中运行的程序来说，这非常有效。但是，假设程序的需求发生了变化——正如我上面提到的那样——现在我需要从game_logic函数中执行一些I/O(例如，使用一个套接字)。例如，如果我试图构建一个大型多人在线游戏，其中状态转换由网格状态和通过Internet与其他玩家的通信组合决定，那么这可能是必需的。

```python
def game_logic(state, neighbors):
    ...
    # Do some blocking input/output in here:
    data = my_socket.recv(100)
    ...
```

​		这种方法的问题是它会减慢整个程序的速度。如果所需的I / O延迟100毫秒(即,一个相当好的越野、往返延迟在互联网上),然后有45网格中的细胞,每一代将最低的4.5秒的评价,因为每个细胞是串行处理的模拟功能。这样做太慢了，会让游戏无法玩。它的伸缩性也很差:如果后来想将网格扩展到10,000个单元，将需要超过15分钟来评估每一代。

​		解决方案是并行执行I/O，因此无论网格有多大，每次生成大约需要100毫秒。为每个工作单元(在本例中是单元格)生成并发执行行的过程称为扇出。等待所有这些并发的工作单元完成，然后才能进入协调流程的下一个阶段(在本例中是生成)，这称为扇入。

​	Python提供了许多内置工具来实现扇形输出和扇形输入，并进行了各种权衡。应该了解每种方法的优缺点，并根据具体情况选择最适合这项工作的工具。请看下面的项目，以了解基于这个生命游戏示例程序的详细信息(参见第57、第58、第60条).

**要点**

* 当一个程序的范围和复杂性增加时，它通常需要多个并发执行行
* 最常见的并发协调类型是扇出(生成新的并发单元)和扇入(等待现有并发单元完成)
* Python有许多不同的方式来实现扇出和扇入