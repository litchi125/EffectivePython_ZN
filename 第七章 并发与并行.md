# 第七章 并发与并行

​		并发性能够让计算机看起来同时多很多事情。例如，在一台只有一个CPU内核的计算机上，如果有哪个程序在单处理器上运行，操作系统就会迅速变化。这样做时，操作系统交错地执行程序，这就展示了一种多个程序同时进行的假象。

​		相反，并行性涉到在同一时间做许多不同的事情。具有多个CPU内核的计算机可以同时执行多个程序。每个CPU内核运行一个独立程序的指令，这允许每个程序在同一时刻前进。

​		在单个程序中，并发性是一种使程序员更容易解决某些类型问题的工具。并发程序支持许多不同的执行路径，包括独立的I/O流，以一种似乎同时和独立的方式向前推进。

​		并行性和并发性之间的关键区别在于加速。当一个程序中有两条不同的路径并行前进时，完成全部工作所需要的时间就减少了一半；执行速度快了两倍。相比之下，并发程序可能会运行数千条看似并行的独立执行路径，但对总体工作没有提供加速。

​		Python有多种风格方式能够使得并发程序变得很容易写。线程支持相对较少的并发性，而协程支持大量的并发函数。Python还可以通过系统调用、子进程和C扩展完成并行工作。但是让并发的Python代码真正并行运行是非常困难的。了解如何在这不同的情况下最好的利用Python是最重要的。

## 第52条 使用subprocess管理子进程

​		Python有运行和管理子进程的可靠库。这使得它成为其他工具(如命令行实用程序)结合在一起的一种很好的语言。当现有的shell脚本变得复杂时(随着时间的推移，他们经常会变得复杂)，出于可读性和可维护性的考虑，将他们分别使用Python重写是一个自然的选择。

​		由Python启动子进程能够并行运行，是我们能够使用Python消耗机器的所有CPU内核，并最大限度地提高程序的吞吐量。虽然Python本身可能受CPU限制(参见第53条)，但使用Python来驱动和协调CPU密集型工作的负载是很容易的。

​		Python有很多方式来运行子进程(例如，os.open，os.exex*)，但是管理子进程的最佳选择是使用子进程内置模块。用子进程运行子进程非常简单。这里，使用模块的run convenience函数来启动进程，读取其输出，并验证它是否终止彻底:

```python
import subprocess
# Enable these lines to make this example work on Windows
# import os
# os.environ['COMSPEC'] = 'powershell'

result = subprocess.run(
    ['echo', 'Hello from the child!'],
    capture_output=True,
    # Enable this line to make this example work on Windows
    # shell=True,
    encoding='utf-8', shell=True)

result.check_returncode()  # No exception means it exited cleanly
print(result.stdout)
>>>
"Hello from the child!"
```

***

**注意**

​		本项目中的示例假设系统存在echo、sleep、OpenSSL命令可用。在Windows上，情况并非如此。关于如何在Windows上运行这些代码片段，参阅完整示例代码。

***

​		子进程独立于父进程(Python解析器)而运行。如果使用popen类而不是run函数创建一个子进程，可以在Python做其他工作时轮询子进程。

~~~python
proc = subprocess.Popen(['sleep', '1'], shell=True)
while proc.poll() is None:
    print('Working...')
    # Some time-consuming work here
	...
print('Exit status', proc.poll())
>>>
Working...
Working...
Working...
Working...
Working...
Working...
...
~~~

​		将子进程与父进程解耦可释放父进程以并行地运行许多子进程。在这里，通过使用Popen提前启动所有的子进程来做到这一点：

~~~python
import time
start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell = True)
    sleep_procs.append(proc)
~~~

​		之后，等待他们完成I/O，用通信方法结束：

~~~python
import time

start = time.time()
sleep_procs = []
for _ in range(10):
    proc = subprocess.Popen(['sleep', '1'], shell=True)
    sleep_procs.append(proc)

for proc in sleep_procs:
    proc.communicate()
end = time.time()
delta = end - start
print(f'Finished in {delta:.3} seconds')
>>>
Finished in 0.111 seconds
~~~

​		如果这些进程按照顺序进行，总的延迟将会是10s或更多，而不是测试的结果。

​		还可以将数据从Python程序传输到子进程，并检查其输出。这允许使用许多其他程序并行运行工作。例如，假设要使用OpenSSL命令工具加密一些数据。使用命令行参数和I/O管道启动子进程很简单。

~~~python
import os
# On Windows, after installing OpenSSL, you may need to
# alias it in your PowerShell path with a command like:
# $env:path = $env:path + ";C:\Program Files\OpenSSL-Win64\bin"

def run_encrypt(data):
    env = os.environ.copy()
    env['password'] = 'zf7ShyBhZOraQDdE/FiZpm/m/8f9X+M1'
    proc = subprocess.Popen(
        ['openssl', 'enc', '-des3', '-pass', 'env:password'],
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
    	shell=True)
    proc.stdin.write(data)
    proc.stdin.flush()  # Ensure that the child gets input
    return proc

~~~

​		在这里，将随机字节通过管道传输到加密函数中，但实际上，这个输入管道将从用户输入、文件句柄、网络套接字等输入数据:

~~~python
procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_encrypt(data)
    procs.append(proc)
~~~

​		子进程并行运行并使用它们的输入。在这里，等待它们完成后，然后检索它们的最终输出。输出是随机的：

~~~python
for proc in procs:
    out, _ = proc.communicate()
    print(out[-10:])
>>>
b't\xcb|j\x8b\xf0\x96P\x85\xa0'
b'\xd5M\xd1.\xa5\xb4C\xb0\xd3_'
b'3\xc4]\x07YB\xd7X\xf1\xd8'

~~~

​		还可以创建并行进程链，就像UNIX管道一样，将一个子进程的输出连接到另一个子进程的输入，等等。下面是一个函数，它将openssl命令行工具作为子进程启动，以生成输入流的Whirlpool散列：

~~~python
def run_hash(input_stdin):
    return subprocess.Popen(
        ['openssl', 'dgst', '-whirlpool', '-binary'],
        stdin=input_stdin,
        stdout=subprocess.PIPE,
    	shell=True)
~~~

​		现在，可以启动一组进程来加密一些数据，然后启动另一组进程来对其加密的输出进行散列。这里必须注意，启动子进程管道的Python解释器进程是如何保留上游进程的stdout实例的:

~~~python
encrypt_procs = []
hash_procs = []
for _ in range(3):
    data = os.urandom(100)

    encrypt_proc = run_encrypt(data)
    encrypt_procs.append(encrypt_proc)

    hash_proc = run_hash(encrypt_proc.stdout)
    hash_procs.append(hash_proc)

    # Ensure that the child consumes the input stream and
    # the communicate() method doesn't inadvertently steal
    # input from the child. Also lets SIGPIPE propagate to
    # the upstream process if the downstream process dies.
    encrypt_proc.stdout.close()
    encrypt_proc.stdout = None
~~~

​		一旦启动子进程，它们之间的I/O将自动发生。所需要做的就是等待他们完成并打印最终输出:

```python
for proc in encrypt_procs:
    proc.communicate()
    assert proc.returncode == 0

for proc in hash_procs:
    out, _ = proc.communicate()
    print(out[-10:])
    assert proc.returncode == 0
>>>
b'\xe2j\x98h\xfd\xec\xe7T\xd84'
b'\xf3.i\x01\xd74|\xf2\x94E'
b'5_n\xc3-\xe6j\xeb[i'
```

​		如果担心子进程永远不会完成或以某种方式阻塞输入或输出管道，可以将timeout参数传递给communicate方法。如果子进程没有在指定的时间内完成，就会引发一个异常，从而可以终止行为不正常的子进程:

```python
# Use this line instead to make this example work on Windows
# proc = subprocess.Popen(['sleep', '10'], shell=True)
proc = subprocess.Popen(['sleep', '10'], shell=True)
try:
    proc.communicate(timeout=0.1)
except subprocess.TimeoutExpired:
    proc.terminate()
    proc.wait()

print('Exit status', proc.poll())
```

**要点**

* 使用子进程模块来运行子进程并管理它们的输入和输出流
* 子进程与Python解释器并行运行，使得能够最大限度地利用CPU内核
* run便利函数的简单使用，Popen类的高级使用，如unix风格的管道
* 使用通信方法的timeout参数来避免死锁和子进程挂起

## 第53条 使用线程阻塞I/O以避免并行

​		**Python的标准实现成为CPython。CPython以两步运行Python。首先，他将源文本解析并编译成字节码，这是作为8位指令的程序的低级表示。(然而，从Python3.6开始，他在技术上是带有16位指令的字节码，但其意思是相同的。)然后，CPython使用基于堆栈的解析器运行字节码。字节码解释器的状态必须在Python程序执行时保持一致。CPython通过一种称为全局解析器(GIL)的机制来加强一致性。**

​		本质上，GIL是一个互斥锁，它防止CPython受到抢占式多线程影响，抢占式多线程是指一个线程通过中断另一个线程来控制一个程序。这样的中断如果发生在意外的时间，可能会破坏解释器状态(例如，垃圾收集引用计数)。GIL防止了这些中断，并确保每个字节码指令与CPython实现及其C扩展模块正常工作。

​		GIL有一个重要的副作用。对于用c++或Java这样的语言编写的程序，拥有多个执行线程意味着一个程序可以同时使用多个CPU内核。尽管Python支持多个执行线程，但GIL每次只会让其中一个线程取得进展。这意味着当使用线程来进行并行计算并加速Python程序时，并不一定如你所愿。

​		例如，要使用Python进行一些计算密集型的工作。这里使用一个简单的数据分解操作代理执行：

~~~python
def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i
~~~

​		分解一组连续的数字需要相当长的时间：

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


numbers = [2139079, 1214759, 1516637, 1852285]
start = time.time()
for number in numbers:
    list(factorize(number))
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.555 seconds
```

​		使用多线程来完成这个计算在其他语言是有意义的，因为可以利用计算机的所有CPU内核。这里用Python试试。定义一个Python线程来执行与之前相同的计算：

```python
from threading import Thread
class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number
    def run(self):
        self.factors = list(factorize(self.number))
```

​	然后，我启动一个线程来并行分解每个数字:

```python
start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
```

​	最后，我等待所有线程完成:

```python
import time


def factorize(number):
    for i in range(1, number + 1):
        if number % i == 0:
            yield i


from threading import Thread


class FactorizeThread(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number

    def run(self):
        self.factors = list(factorize(self.number))


start = time.time()
threads = []
numbers = [2139079, 1214759, 1516637, 1852285]
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.480 seconds
```

​		令人惊讶的是，这比串行运行中运行factorize话费的时间并不少多少。对于每个数字一个线程，由于创建线程和协调线程的开销，可能并不会像其他语言那样提高四倍的速率。可能希望在用来运行此代码的双核机器上只有2倍的速度。但是，当需要使用多个cpu时，这些线程的性能不会更好。这演示了GIL(例如，锁争用和调度开销)对在标准CPython解释器中运行的程序的影响。

​		有些方法可以让CPython使用多个核从而大幅度提高运行效率，但他们不能与标准的Thread类一起工作(参见第64条)。既然有这些限制，为什么Python还要支持多线程呢？有两个很重要的原因。

​		第一、多线程让程序很容易在同一时间看上去并行工作。同时处理多项任务是很难自己做到的(参见第56条)。有了多线程，就可以让Python并发运行函数。这是可行的，因为CPython确保了Python执行的线程之间一定程度的公平性，即使GIL，每次只有一个线程取得进展。

​		第二、是为了处理I/O阻塞，当Python执行某些类型的系统操作时，阻塞I/O会发生。

​		Python程序使用系统调用来完成计算机的操作系统与外部环境进行交互。阻塞I/O包括读取和写入文件、与网络交互、与显示器等设备通信等。线程将程序与操作系统响应请求所花费的时间隔离开来，从而帮助处理阻塞I/O。

​		例如，假设要通过串口向远程控制的直升机发送一个信号。将使用一个缓慢的系统调用作为这个活动的代理。这个函数要求操作系统阻塞0.1秒，然后将控制放回给程序，这类似于同步串行端口发生的情况：

~~~python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)
~~~

​		串行运行这个系统调用需要线性增加的时间:

```python
import select
import socket
def slow_systemcall():
    select.select([socket.socket()], [], [], 0.1)

start = time.time()
for _ in range(5):
    slow_systemcall()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.502 seconds
```

​		这样做的问题是，当slow_systemcall函数正在运行时，程序不能取得任何进展。程序执行的主线程被阻塞在select系统调用上。这种情况在实践中是非常可怕的。当向计算机发送信号时，需要能够计算出它的下一步行动；否则，它会崩溃。当发现需要同时执行阻塞I/O和计算时，就应该考虑将系统调用转移到线程中。

​		在这里，在单独的线程中运行多个slow_systemcall函数的调用。这将允许与多个串行端口(和直升机)在同一时间，同时离开主线程做任何需要的计算:

```python
start = time.time()
threads = []
for _ in range(5):
    thread = Thread(target=slow_systemcall)
thread.start()
threads.append(thread)
```

​		随着线程开始，这里做了一些工作来计算下一个直升机移动之前等待系统调用线程完成:

```
def compute_helicopter_location(index):
    ...
for i in range(5):
    compute_helicopter_location(i)
for thread in threads:
    thread.join()
end = time.time()
delta = end - start
print(f'Took {delta:.3f} seconds')
>>>
Took 0.108 seconds
```

​		**并行时间比串行时间小~5倍。这表明所有的系统调用都将从多个Python线程并行运行，即使它们受到GIL的限制。GIL阻止我的Python代码并行运行，但是它对系统调用没有影响。这是因为Python线程在进行系统调用之前释放GIL，并在系统调用完成后重新获取GIL。**

​		除了使用线程之外，还有许多其他方法来处理阻塞I/O，比如asyncio内置模块，这些替代方法有重要的好处。但是这些选项可能需要在重构代码以适应不同的执行模型时进行额外的工作(参见第60条和第62条)。使用线程是并行执行阻塞I/O的最简单方法，同时对程序进行最小的更改。

**要点**

* 由于全局解释器锁(GIL)， Python线程不能在多个CPU内核上并行运行
* 尽管有GIL, Python线程仍然是有用的，因为它们提供了一种简单的方法，似乎可以同时做多个事情
* 使用Python线程并行进行多个系统调用。这允许您在进行计算的同时进行阻塞I/O 

## 第54条 使用lock防止线程中的数据竞争

​		在学习了全局解释器锁(GIL)之后(参见第53条)，许多新的Python程序员认为他们可以完全放弃在他们的代码中使用互斥锁(也称为互斥锁)。如果GIL已经在阻止Python线程在多个CPU内核上并行运行，那么它还必须充当程序数据结构的锁，对吗?对列表和字典等类型的一些测试甚至可能显示这一假设似乎成立。

​		但要注意，事实并非如此。GIL不会保护你的。尽管一次只有一个Python线程运行，但线程对数据结构的操作可以在Python解释器中的任何两个字节码指令之间中断。如果同时从多个线程访问相同的对象，这是危险的。由于这些中断，数据结构的不变量实际上随时都可能被违反，使程序处于损坏状态。

​		例如，假设想编写一个并行计算许多事情的程序，比如从整个传感器网络中采样光级。如果想要确定一段时间内轻样本的总数，可以用一个新类来聚合它们：

​		假设每个传感器都有自己的工作线程，因为从传感器读取数据需要阻塞I/O。在每个传感器测量之后，工作线程将计数器增加到期望的最大读数:

​		在这里，为每个传感器并行运行一个工作线程，并等待它们完成它们的读数:

```python
class Counter:
    def __init__(self):
        self.count = 0

    def increment(self, offset):
        self.count += offset

def worker(sensor_index, how_many, counter):
    for _ in range(how_many):
        # Read from the sensor
        ...
        counter.increment(1)

from threading import Thread
how_many = 10**5
counter = Counter()
threads = []
for i in range(5):
    thread = Thread(target=worker,
                    args=(i, how_many, counter))
    threads.append(thread)
    thread.start()
for thread in threads:
    thread.join()
expected = how_many * 5
found = counter.count
print(f'Counter should be {expected}, got {found}')
>>>
Counter should be 500000, got 415471
```

​		这看起来很简单，结果也应该很明显，但结果却大相径庭!这里发生了什么?这么简单的事情怎么会出这么大的问题，尤其是在一次只能运行一个Python解释器线程的情况下?

​		Python解释器强制执行所有线程之间的公平性，以确保它们得到大致相同的处理时间。为此，Python在线程运行时暂停一个线程，并依次恢复另一个线程。问题是不知道Python什么时候会挂起线程。线程甚至可以在看似原子操作的过程中中途暂停。这就是本案的情况。

​		Counter对象的increment方法体看起来很简单，从工作线程的角度来看，相当于下面的语句:

~~~python
counter.count += 1
~~~

​		但是对象属性上使用的+=操作符实际上指示Python在后台执行三个独立的操作。上面的语句相当于:

~~~python
value = getattr(counter, 'count')
result = value + 1
setattr(counter, 'count', result)
~~~

​		递增计数器的Python线程可以挂起在上述任意两个操作之间。如果操作的交错方式导致将旧版本的值赋给计数器，那么这就有问题了。以下是两个线程(A和B)之间糟糕交互的例子:

```python
# Running in Thread A
value_a = getattr(counter, 'count')
# Context switch to Thread B
value_b = getattr(counter, 'count')
result_b = value_b + 1
setattr(counter, 'count', result_b)
# Context switch back to Thread A
result_a = value_a + 1
setattr(counter, 'count', result_a)
```

​		线程B在线程A完全完成之前中断了它。线程B运行并结束，但是线程A在执行过程中恢复，覆盖了线程B的计数器增量的所有进程。这正是上面光传感器的例子中所发生的。

​		为了防止类似的数据竞争和其他形式的数据结构损坏，Python在线程内置模块中包含了一组健壮的工具。其中最简单也是最有用的是Lock类，这是一个互斥锁(互斥锁)。

​		通过使用锁，可以让Counter类保护其当前值不受多个线程的同时访问。每次只有一个线程能够获得锁。在这里，使用一个with语句来获取和释放锁; 这使得当锁被持有时，更容易看到哪个代码正在执行(参见第66条):

```python
from threading import Lock
class LockingCounter:
    def __init__(self):
        self.lock = Lock()
        self.count = 0
    def increment(self, offset):
        with self.lock:
            self.count += offset
```

​		现在，像以前一样运行工作线程，但是使用一个LockingCounter

```python
from threading import Lock
class LockingCounter:
    def __init__(self):
        self.lock = Lock()
        self.count = 0
    def increment(self, offset):
        with self.lock:
            self.count += offset

counter = LockingCounter()
for i in range(5):
    thread = Thread(target=worker,
                    args=(i, how_many, counter))
    threads.append(thread)
    thread.start()
for thread in threads:
    thread.join()
expected = how_many * 5
found = counter.count
print(f'Counter should be {expected}, got {found}')
>>>
Counter should be 500000, got 500000
```

​		结果正是我所期望的。lock解决了这个问题。

**要点**

* 即使Python有一个全局解释器锁，仍然要防止程序中线程之间的数据竞争
* 如果允许多个线程在没有互斥锁(互斥锁)的情况下修改相同的对象，程序将破坏它们的数据结构
* 使用threading内置模块中的Lock类在多个线程之间强制执行程序的不变

