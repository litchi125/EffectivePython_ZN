# 第八章 稳定与性能

​		一旦写好了一个非常有用的程序，下一步就是让代码更加高效、且使之更加不易受攻击。使程序在遇到以外情况时变得更加可靠与确保程序的正确性一样重要。Python有内置的特性和模块，有助于强化程序，使他们在各种情况下都很健壮。

​		健壮性的一个方面是可伸缩性和性能。当在实现处理大量数据的Python程序时，经常会看到由于代码的算法复杂性或其他类型的计算开销而导致的速度变慢。幸运的是，Python包含了许多算法和数据结构，需要用最少的努力来实现高性能。

## 第65条 合理利用try/except/else/finally结构中的每个代码块

​		在Python中，处理异常通常包括四个步。这些是在try、except、else和finally块的功能中捕获的。在复合语句中，每个块都有独特的用途，它们的各种组合都很有用(参见第87条。

**finally Blocks**

​		如果希望捕获异常后，异常向上传播且发生异常时运行清理代码，那就使用try/finally。try/finally的一个常见用法是用于可靠地关闭文件句柄(参见第66条)：

```python
def try_finally_example(filename):
    print('* Opening file')
    handle = open(filename, encoding='utf-8') # May raise OSError
    try:
        print('* Reading data')
        return handle.read()  # May raise UnicodeDecodeError
    finally:
        print('* Calling close()')
        handle.close()        # Always runs after try block
```

​		read方法引发的任何异常都将一直传播到try_finally_example方法调用处，但finally块中handle的close方法将首先运行:

```python
try:
    filename = 'random_data.txt'
    
    with open(filename, 'wb') as f:
        f.write(b'\xf1\xf2\xf3\xf4\xf5')  # Invalid utf-8
    
    data = try_finally_example(filename)
    # This should not be reached.
    import sys
    sys.exit(1)
except:
    logging.exception('Expected')
else:
    assert False
>>>
>>>
* Opening file
* Reading data
* Calling close()
Traceback ...
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in
➥position 0: invalid continuation byte
```

​		必须在try块之前调用open，因为当打开文件时发生的异常(如文件不存在时的OSError)应该完全跳过finally块:

~~~python
try_finally_example('does_not_exist.txt')
>>>
* Opening file
Traceback ...
FileNotFoundError: [Errno 2] No such file or directory:
➥'does_not_exist.txt'
~~~

**else Blocks**

​		在处理异常时使用try/except/else，在出现异常时能够清晰的指导哪个异常会被向上抛出。当try代码块内有抛出异常时，else代码块就会运行。else块可以最小化try块中的代码量，这有助于隔离潜在的异常原因并提高可读性。例如，说想从一个字符串加载JSON字典数据，并返回它包含的键值:

```python
import json

def load_json_key(data, key):
    try:
        print('* Loading JSON data')
        result_dict = json.loads(data)  # May raise ValueError
    except ValueError as e:
        print('* Handling ValueError')
        raise KeyError(key) from e
    else:
        print('* Looking up key')
        return result_dict[key]         # May raise KeyError
```

​		顺利的情况下，json数据在try代码块中解码，然后在else代码块中进行键查找：

```python
print(load_json_key('{"foo": "bar"}', 'foo'))
>>>
* Loading JSON data
* Looking up key
bar
```

​		如果输入不是一个有效的json，json.loads解码时就会抛出ValueError异常。这个异常被except捕获并抛出：

~~~python
load_json_key('{"foo": bad payload', 'foo')
>>>
* Loading JSON data
* Handling ValueError
Traceback ...
JSONDecodeError: Expecting value: line 1 column 9 (char 8)
The above exception was the direct cause of the following
➥exception:
Traceback ...
KeyError: 'foo'
~~~

​		如果键查找引发任何异常，他们将传播到调用方，因为他们在try代码块之外。else子句确保try/except后面的内容在视觉上与except不同。这使得异常传播行为更加清晰：

```python
try:
    load_json_key('{"foo": "bar"}', 'does not exist')
except:
    logging.exception('Expected')
else:
    assert False
>>>
>>>
* Loading JSON data
* Looking up key
Traceback ...
KeyError: 'does not exist'
```

**Everything Together**

​	  如果想在一个符合语句中完成所有操作，使用try/except/else/finally结构。例如，假设想从文件中读取要做的工作描述，处理，然后更新文件。这里，try快用于读取文件并处理它；except用于处理来自期望的try的异常；else用于更新文件，并允许相关异常向上传播；finally清除文件句柄。

```python
UNDEFINED = object()
DIE_IN_ELSE_BLOCK = False

def divide_json(path):
    print('* Opening file')
    handle = open(path, 'r+')   # May raise OSError
    try:
        print('* Reading data')
        data = handle.read()    # May raise UnicodeDecodeError
        print('* Loading JSON data')
        op = json.loads(data)   # May raise ValueError
        print('* Performing calculation')
        value = (
            op['numerator'] /
            op['denominator'])  # May raise ZeroDivisionError
    except ZeroDivisionError as e:
        print('* Handling ZeroDivisionError')
        return UNDEFINED
    else:
        print('* Writing calculation')
        op['result'] = value
        result = json.dumps(op)
        handle.seek(0)          # May raise OSError
        if DIE_IN_ELSE_BLOCK:
            import errno
            import os
            raise OSError(errno.ENOSPC, os.strerror(errno.ENOSPC))
        handle.write(result)    # May raise OSError
        return value
    finally:
        print('* Calling close()')
        handle.close()          # Always runs
```

​		没有异常的情况下，try、else、finally运行：

~~~python
temp_path = 'random_data.json'

with open(temp_path, 'w') as f:
    f.write('{"numerator": 1, "denominator": 10}')

print(divide_json(temp_path))
>>>
* Opening file
* Reading data
* Loading JSON data
* Performing calculation
* Writing calculation
* Calling close()
0.1
~~~

​		如果有异常的情况，try、except、finally运行：

```python
temp_path = 'random_data.json'

with open(temp_path, 'w') as f:
    f.write('{"numerator": 1, "denominator": 0}')

print(divide_json(temp_path))
```

​		如果json文件格式正确，try运行并且抛出了一个异常，finally运行，异常就会向上跑给调用者。except和else不运行：

```python
try:
    with open(temp_path, 'w') as f:
        f.write('{"numerator": 1 bad data')

    divide_json(temp_path)
except:
    print('Expected')
else:
    assert False
>>>
* Opening file
* Reading data
* Loading JSON data
* Calling close()
Traceback ...
JSONDecodeError: Expecting ',' delimiter: line 1 column 17
➥(char 16)
```

​		这种布局特别有用，因为所有的快都以直观的方式一起工作。例如，这里通过耗尽磁盘空间的同时运行divide_json函数来模拟这一点：

```python
try:
    with open(temp_path, 'w') as f:
        f.write('{"numerator": 1, "denominator": 10}')
    DIE_IN_ELSE_BLOCK = True
    
    divide_json(temp_path)
except:
    logging.exception('Expected')
else:
    assert False
    
>>>
* Opening file
* Reading data
* Loading JSON data
* Performing calculation
* Writing calculation
* Calling close()
Traceback ...
OSError: [Errno 28] No space left on device

```

​		当重写结果数据时在else块中引发异常时，finally块仍然运行并按预期关闭文件句柄。

**要点**

* try/finally复合语句允许运行清理代码，而不管try块中是否引发了异常
* else块可以最小化try块中的代码量，并从视觉上区分成功案例与try/except块
* else块可用于在成功的try块之后，但在finally块的普通清理之前执行其他操作

## 第66条 虑用contextlib和with语句来改写可复用的try/finally代码

 		Python中的with语句用于指示代码何时在特定上下文中运行。例如，互斥锁(参见第54条)可以用在with语句中，表示缩进代码块只在锁被持有时运行:

```python
from threading import Lock

lock = Lock()
with lock:
    # Do something while maintaining an invariant
    pass
```

​		上面的例子等价于使用try/finally结构，因为Lock类正确地启用了with语句：

```python
lock.acquire()
try:
    # Do something while maintaining an invariant
    pass
finally:
    lock.release()
```

​		这个版本的with语句更好，因为它消除了编写try/finally构造的重复代码的需要，并且它确保您不会忘记对每个acquire调用有一个相应的release调用。

​		通过使用contextlib内置模块，可以很容易地让对象和函数与语句一起工作。这个模块包含了contextmanager装饰器(参见第26条)，它允许在with语句中使用一个简单的函数。这比用特殊方法--enter--和--exit--(标准方法)定义一个新类容易得多。

​		例如，假设希望某个代码区域有时有更多的调试日志记录。这里，定义了一个函数，它在两个严重级别上进行日志记录:

```python
import logging
logging.getLogger().setLevel(logging.WARNING)

def my_function():
    logging.debug('Some debug data')
    logging.error('Error log here')
    logging.debug('More debug data')
```

​		程序的默认log等级是WARRING，在运行代码时只有错误消息会打印出来：

~~~python
my_function()
>>>
Error log here
~~~

​		可以通过定义上下文管理器临时提高这个函数的日志级别。这个helper函数在运行with块中的代码之前提高了日志记录的严重级别，之后降低了日志记录的严重级别:

```python
from contextlib import contextmanager

@contextmanager
def debug_logging(level):
    logger = logging.getLogger()
    old_level = logger.getEffectiveLevel()
    logger.setLevel(level)
    try:
        yield
    finally:
        logger.setLevel(old_level)
```

​		yield表达式是with块内容将执行的点(参见第30条)。在with块中发生的任何异常都将由yield表达式重新引发，以便在helper函数中捕获(参见第35条)。

​		现在，可以再次调用相同的日志记录函数，但是是在debug_logging上下文中。这一次，所有的调试消息都在with块期间被打印到屏幕上。在with块之外运行的相同函数不会打印调试消息:

~~~python
with debug_logging(logging.DEBUG):
    print('* Inside:')
    my_function()

print('* After:')
my_function()
>>>
* Inside:
* After:
DEBUG:root:Some debug data
ERROR:root:Error log here
DEBUG:root:More debug data
ERROR:root:Error log here
~~~

**Using with Targets**

​		传递给with语句的上下文管理器也可以返回一个对象。该对象被赋值给复合语句的as部分中的一个局部变量。这使得在with块中运行的代码能够直接与其上下文交互。

​		例如，假设想写一个文件，并确保它总是正确关闭。可以通过将open传递给with语句来做到这一点。Open为with的as目标返回一个文件句柄，并在with块退出时关闭该句柄:

~~~python
with open('my_output.txt', 'w') as handle:
	handle.write('This is some data!')
~~~

​		这种方法比每次手动打开和关闭文件句柄更python化。当执行离开with语句时，文件最终是关闭的。通过突出显示关键部分，它还鼓励减少在打开文件句柄时执行的代码数量，这通常是一种良好的实践。

​		要使自己的函数能够为目标提供值，所需要做的就是从上下文管理器中生成一个值。例如，这里定义了一个上下文管理器来获取Logger实例，设置其级别，然后将其作为目标:

```python
@contextmanager
def log_level(level, name):
    logger = logging.getLogger(name)
    old_level = logger.getEffectiveLevel()
    logger.setLevel(level)
    try:
        yield logger
    finally:
        logger.setLevel(old_level)
```

​		调用as目标上的debug等日志记录方法会产生输出，因为特定Logger实例上的with块中的日志记录严重级别设置得足够低。直接使用日志模块不会打印任何东西，因为默认程序记录器的默认日志级别是WARNING:

```python
with log_level(logging.DEBUG, 'my-log') as logger:
    logger.debug(f'This is a message for {logger.name}!')
    logging.debug('This will not print')
>>>
This is a message for my-log!
```

​		在with语句退出后，在名为“my-log”的Logger上调用调试日志记录方法将不会打印任何内容，因为默认的日志记录严重级别已经恢复。错误日志信息将始终打印:

```python
logger = logging.getLogger('my-log')
logger.debug('Debug will not print')
logger.error('Error will print')
>>>
Error will print
```

​		稍后，可以通过更新with语句来更改想要使用的记录器的名称。这将指向与块中的作为目标的Logger到一个不同的实例，但我不需要更新任何其他代码来匹配:

```python
with log_level(logging.DEBUG, 'other-log') as logger:
    logger.debug(f'This is a message for {logger.name}!')
    logging.debug('This will not print')
>>>
DEBUG:other-log:This is a message for other-log!
```

​		这种状态隔离以及创建上下文和在该上下文中操作之间的分离是with语句的另一个好处。

**要点**

* with语句允许重用try/finally块中的逻辑，减少视觉干扰
* 内置模块contextlib提供了一个contextmanager装饰器，可以方便地在with语句中使用自己的函数
* 上下文管理器生成的值提供给with语句的as部分。它有助于让代码直接访问特殊上下文的原因

## 第67条 用datetime模块处理本地时间，不要用time模块

​		协调世界时(UTC)是标准的、与时区无关的时间表示。UTC对于将时间表示为自UNIX纪元以来的秒数的计算机非常有效。但是UTC对人类来说并不理想。人类会根据他们当前所处的位置来参考时间。人们说“中午”或“上午8点”，而不是“UTC 15:00 - 7小时。”如果程序处理时间，可能会发现为了便于人们理解，需要在UTC和本地时钟之间转换时间。

​		Python提供了两种实现时区转换的方法。使用time内置模块的旧方法非常容易出错。使用datetime内置模块的新方法在名为pytz的社区构建包的帮助下工作得非常好。

​		应该熟悉time和datetime，以便彻底理解为什么datetime是最好的选择，并且应该避免使用time。

*The time Module*

​		时间内置模块中的localtime函数允许将UNIX时间戳(UTC中UNIX纪元以来的秒数)转换为与主机计算机时区匹配的本地时间(在我的例子中是太平洋夏令时)。这个本地时间可以使用strftime函数以人类可读的格式打印出来:

```python
import time

now = 1552774475
local_tuple = time.localtime(now)
time_format = '%Y-%m-%d %H:%M:%S'
time_str = time.strftime(time_format, local_tuple)
print(time_str)
>>>
2019-03-17 06:14:35
```

​		通常还需要采用另一种方法，从用户输入人类可读的本地时间开始，然后将其转换为UTC时间。可以通过使用strptime函数来解析时间字符串，然后调用mktime将本地时间转换为UNIX时间戳来实现这一点:

```python
time_tuple = time.strptime(time_str, time_format)
utc_now = time.mktime(time_tuple)
print(utc_now)
>>>
1552774475.0
```

​		最初可能假设可以直接操作time、localtime和strptime函数的返回值来进行时区转换。但这是个非常糟糕的主意。时区会因为当地的法律而随时改变。这对你自己来说太复杂了，尤其是如果你想要处理全球每个城市的航班起飞和到达。

​		许多操作系统都有与时区变化自动同步的配置文件。如果平台支持，Python允许您通过时间模块使用这些时区。在其他平台上，比如Windows，有些时区功能根本不能从时间开始使用。例如，这里解析了一个从旧金山时区出发的时间，太平洋夏令时(PDT):

```python
import os

if os.name == 'nt':
    print("This example doesn't work on Windows")
else:
    parse_format = '%Y-%m-%d %H:%M:%S %Z'
    depart_sfo = '2019-03-16 15:45:16 PDT'
    time_tuple = time.strptime(depart_sfo, parse_format)
    time_str = time.strftime(time_format, time_tuple)
    print(time_str)
>>>
This example doesn't work on Windows
```

​		在看到'PDT'与strptime函数一起工作之后，可能还会假设计算机知道的其他时区也可以工作。不幸的是，事实并非如此。strptime在看到东部夏令时(EDT)时引发异常，这是纽约的时区:

```python
arrival_nyc = '2019-03-16 23:33:24 EDT'
time_tuple = time.strptime(arrival_nyc, time_format)
>>>
Traceback ...
ValueError: unconverted data remains: EDT
```

​		这里的问题是时间模块的平台依赖性。它的行为是由底层C函数与宿主操作系统的工作方式决定的。这使得Python中的time模块的功能不可靠。时间模块在多个本地时间无法正常工作。因此，您应该避免将时间模块用于此目的。如果必须使用时间，请仅在UTC和主机本地时间之间使用它。对于所有其他类型的转换，请使用datetime模块。

*The datetime Module*

​		Python中表示时间的第二个选项是datetime内建模块中的datetime类。与time模块类似，datetime可用于将UTC中的当前时间转换为本地时间。

​		这里，将UTC的当前时间转换为我的计算机的本地时间，PDT:

```python
from datetime import datetime, timezone

now = datetime(2019, 3, 16, 22, 14, 35)
now_utc = now.replace(tzinfo=timezone.utc)
now_local = now_utc.astimezone()
print(now_local)
>>>
2019-03-17 06:14:35+08:00
```

​		datetime模块也可以很容易地将本地时间转换回UTC格式的UNIX时间戳:

```python
import time
from datetime import datetime, timezone
time_format = '%Y-%m-%d %H:%M:%S'
time_str = '2019-03-16 15:14:35'
now = datetime.strptime(time_str, time_format)
time_tuple = now.timetuple()
utc_now = time.mktime(time_tuple)
print(utc_now)
>>>
1552774475.0

```

​		与time模块不同，datetime模块具有从一个本地时间可靠地转换为另一个本地时间的功能。但是，datetime仅通过其tzinfo类和相关方法为时区操作提供机制。Python的默认安装缺少UTC以外的时区定义。

​		幸运的是，Python社区已经解决了这个问题，可以从Python包索引中下载pytz模块(参见第82条)。Pytz包含可能需要的每个时区定义的完整数据库。

​		为了有效地使用pytz，应该总是首先将本地时间转换为UTC。对UTC值执行任何需要的datetime操作(例如偏移量)。然后，将其转换为本地时间作为最后一步。

​		例如，这里将纽约市航班到达时间转换为UTC日期时间。尽管其中一些调用看起来是多余的，但在使用pytz时，所有这些调用都是必要的:

```python
import pytz

arrival_nyc = '2019-03-16 23:33:24'
nyc_dt_naive = datetime.strptime(arrival_nyc, time_format)
eastern = pytz.timezone('US/Eastern')
nyc_dt = eastern.localize(nyc_dt_naive)
utc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))
print(utc_dt)
>>>
2019-03-17 03:33:24+00:00
```

​		一旦有了UTC日期时间，可以将它转换为旧金山当地时间:

~~~python
pacific = pytz.timezone('US/Pacific')
sf_dt = pacific.normalize(utc_dt.astimezone(pacific))
print(sf_dt)
>>>
2019-03-16 20:33:24-07:00
~~~

​		同样简单的是，可以把它转换成尼泊尔当地时间:

~~~python
nepal = pytz.timezone('Asia/Katmandu')
nepal_dt = nepal.normalize(utc_dt.astimezone(nepal))
print(nepal_dt)
>>>
2019-03-17 09:18:24+05:45

~~~

​		使用datetime和pytz，这些转换在所有环境中都是一致的，而不管主机计算机运行的是什么操作系统.

**要点**

* 避免使用时间模块在不同时区之间进行转换
* 使用datetime内置模块和pytz社区模块来可靠地转换不同时区的时间
* 始终用UTC表示时间，并将转换为本地时间作为显示前的最后一步。

## 第68条 用copyreg实现可靠的pickle操作

​		pickle内置模块可以将Python对象序列化为字节流，并将字节反序列化回对象。pickle的字节流不应该用于不受信任方之间的通信。pickle的目的是让您在您通过二进制通道控制的程序之间传递Python对象。

***

**Note**

​		pickle模块的序列化格式在设计上是不安全的。序列化的数据本质上包含了一个描述如何重构原始Python对象的程序。这意味着恶意的pickle负载可能被用来破坏Python程序中试图反序列化它的任何部分。

​		相反，json模块在设计上是安全的。序列化的JSON数据包含一个对象层次结构的简单描述。反序列化JSON数据不会使Python程序暴露于额外的风险。像JSON这样的格式应该用于程序之间或相互不信任的人之间的通信。

***

​		例如，假设想使用一个Python对象来表示游戏中玩家进程的状态。游戏状态包括玩家所处的关卡以及他们所剩下的生命数:

```python
class GameState:
    def __init__(self):
        self.level = 0
        self.lives = 4
```

​		程序在游戏运行时修改这个对象:

```python
state = GameState()
state.level += 1  # Player beat a level
state.lives -= 1  # Player had to try again

print(state.__dict__)
>>>
{'level': 1, 'lives': 3}
```

​		当用户退出游戏时，程序可以将游戏状态保存到文件中，以便在以后的时间恢复。通过pickle模块可以很容易地做到这一点。在这里，使用dump函数将GameState对象写入文件:

```python
import pickle

state_path = 'game_state.bin'
with open(state_path, 'wb') as f:
    pickle.dump(state, f)
```

​		之后，可以调用文件的加载函数并取回GameState对象，就像它从未被序列化过一样:

~~~python
with open(state_path, 'rb') as f:
	state_after = pickle.load(f)
print(state_after.__dict__)
>>>
{'level': 1, 'lives': 3}
~~~

​		这种方法的问题在于，随着时间的推移，游戏功能的扩展会发生什么。假设想让玩家获得高分。为了追踪玩家的分数，将在GameState类中添加一个新字段：

```python
class GameState:
    def __init__(self):
        self.level = 0
        self.lives = 4
        self.points = 0  # New field
```

​		使用pickle序列化新版本的GameState类将和以前一样工作。在这里，通过序列化一个带有转储的字符串和一个带有加载的对象来模拟遍历一个文件的往返过程:

```python
state = GameState()
serialized = pickle.dumps(state)
state_after = pickle.loads(serialized)
print(state_after.__dict__)
>>>
{'level': 0, 'lives': 4, 'points': 0}
```

​		但是对于用户可能想要恢复的旧保存的GameState对象会发生什么呢?在这里，使用带有GameState类新定义的程序来解pickle一个旧游戏文件:

```python
with open(state_path, 'rb') as f:
    state_after = pickle.load(f)

print(state_after.__dict__)
>>>
{'level': 1, 'lives': 3}
```

​		points属性丢失了。这尤其令人困惑，因为返回的对象是新GameState类的一个实例。

​		这种行为是pickle模块工作方式的副产品。它的主要用例是简化对象序列化。一旦pickle的使用超出了常规用途，模块的功能就会以令人惊讶的方式出现故障。

​		使用copyreg内置模块可以很容易地修复这些问题。copyreg模块允许注册负责序列化和反序列化Python对象的函数，允许控制pickle的行为并使其更可靠。

*Default Attribute Values*

***

​		在最简单的情况下，可以使用带有默认参数的构造函数(参见第23条)来确保GameState对象在unpickling后始终具有所有属性。在这里，这样重新定义构造函数:

```python
class GameState:
    def __init__(self, level=0, lives=4, points=0):
        self.level = level
        self.lives = lives
        self.points = points
```

​		为了使用这个构造函数进行pickle，定义了一个helper函数，它接受一个GameState对象，并将其转换为copyreg模块的参数元组。返回的元组包含用于unpickling的函数和传递给unpickling函数的参数:

```python
def pickle_game_state(game_state):
    kwargs = game_state.__dict__
    return unpickle_game_state, (kwargs,)
```

​		现在，需要定义unpickle_game_state辅助函数。这个函数接受来自pickle_game_state的序列化数据和参数，并返回相应的GameState对象。它是构造函数的一个小小的包装:

```python
def unpickle_game_state(kwargs):
    return GameState(**kwargs)
```

​		现在，可以将这些函数注册到copyreg模块中：

```python
import copyreg

copyreg.pickle(GameState, pickle_game_state)
```

​		注册后，序列化和反序列化工作如常:

```python
state = GameState()
state.points += 1000
serialized = pickle.dumps(state)
state_after = pickle.loads(serialized)
print(state_after.__dict__)
>>>
{'level': 0, 'lives': 4, 'points': 1000}

```

​		完成注册后，现在将再次改变GameState的定义，让玩家能够使用一些魔法咒语。这个改变类似于在GameState中添加点域:

```python
class GameState:
    def __init__(self, level=0, lives=4, points=0, magic=5):
        self.level = level
        self.lives = lives
        self.points = points
        self.magic = magic  # New field
```

​		但与之前不同的是，反序列化一个旧的GameState对象将导致有效的游戏数据，而不是丢失属性。这是可行的，因为unpickle_game_state直接调用GameState构造函数，而不是使用pickle模块的默认行为，即只保存和恢复属于对象的属性。GameState构造函数的关键字参数具有默认值，将用于任何缺少的参数。这将导致旧的游戏状态文件在被反序列化时接收到新的魔法字段的默认值:

~~~python
print('Before:', state.__dict__)
state_after = pickle.loads(serialized)
print('After: ', state_after.__dict__)
>>>
Before: {'level': 0, 'lives': 4, 'points': 1000}
After: {'level': 0, 'lives': 4, 'points': 1000, 'magic': 5}
~~~

*Versioning Classes*

​		有时需要通过删除字段来对Python对象进行向后不兼容的更改。这样做会阻止上面的默认参数方法工作。

​		例如，假设意识到有限的生命数量是个糟糕的想法，想要从游戏中移除生命的概念。在这里，重新定义了GameState类，使其不再拥有lives字段:

```python
class GameState:
    def __init__(self, level=0, points=0, magic=5):
        self.level = level
        self.points = points
        self.magic = magic
```

​		问题是，这破坏了旧游戏数据的反序列化。旧数据中的所有字段，甚至从类中移除的字段，都将通过unpickle_game_state函数传递给GameState构造函数:

~~~python
pickle.loads(serialized)
>>>
Traceback ...
TypeError: __init__() got an unexpected keyword argument
➥'lives'
~~~

​		可以通过向提供给copyreg的函数添加一个版本参数来解决这个问题。当pickle一个新的GameState对象时，新的序列化数据将有一个指定的版本2:

```python
def pickle_game_state(game_state):
    kwargs = game_state.__dict__
    kwargs['version'] = 2
    return unpickle_game_state, (kwargs,)
```

​		旧版本的数据不会有版本参数，这意味着我可以相应地操纵传递给GameState构造函数的参数:

```python
def unpickle_game_state(kwargs):
    version = kwargs.pop('version', 1)
    if version == 1:
        del kwargs['lives']
    return GameState(**kwargs)
```

​		现在，反序列化旧对象可以正常工作:

```python
copyreg.pickle(GameState, pickle_game_state)
print('Before:', state.__dict__)
state_after = pickle.loads(serialized)
print('After: ', state_after.__dict__)
>>>
Before: {'level': 0, 'lives': 4, 'points': 1000}
After: {'level': 0, 'points': 1000, 'magic': 5}
```

​		可以继续使用这种方法来处理同一个类的未来版本之间的更改。将类的旧版本调整为类的新版本所需要的任何逻辑都可以在unpickle_game_state函数中找到。

*Stable Import Paths*

​		使用pickle时可能遇到的另一个问题是重命名类会造成破坏。通常在程序的生命周期中，将通过重命名类并将它们移到其他模块来重构代码。不幸的是，这样做会破坏pickle模块，除非非常小心。

​		在这里，将GameState类重命名为BetterGameState，并从程序中完全删除旧类:

```python
copyreg.dispatch_table.clear()
state = GameState()
serialized = pickle.dumps(state)
del GameState
class BetterGameState:
    def __init__(self, level=0, points=0, magic=5):
        self.level = level
        self.points = points
        self.magic = magic
```

​		试图反序列化一个旧的GameState对象现在失败了，因为类无法找到:

~~~python
pickle.loads(serialized)
>>>
Traceback ...
AttributeError: Can't get attribute 'GameState' on <module
➥'__main__' from 'my_code.py'>
~~~

​		这个异常的原因是序列化对象的类的导入路径被编码在pickle的数据中:

~~~python
print(serialized)
>>>
b'\x80\x04\x95A\x00\x00\x00\x00\x00\x00\x00\x8c\x08__main__
➥\x94\x8c\tGameState\x94\x93\x94)\x81\x94}\x94(\x8c\x05level
➥\x94K\x00\x8c\x06points\x94K\x00\x8c\x05magic\x94K\x05ub.'
~~~

​		解决方案是再次使用copyreg。可以为函数指定一个稳定的标识符，用于解pickle对象。这允许在进行反序列化时将pickle的数据转换为具有不同名称的不同类。它给了一个间接的层次:

~~~python
copyreg.pickle(BetterGameState, pickle_game_state)
~~~

​		在使用copyreg之后，可以看到unpickle_game_state的导入路径被编码在序列化的数据中，而不是BetterGameState:

~~~python
state = BetterGameState()
serialized = pickle.dumps(state)
print(serialized)
>>>
b'\x80\x04\x95W\x00\x00\x00\x00\x00\x00\x00\x8c\x08__main__
➥\x94\x8c\x13unpickle_game_state\x94\x93\x94}\x94(\x8c
➥\x05level\x94K\x00\x8c\x06points\x94K\x00\x8c\x05magic\x94K
➥\x05\x8c\x07version\x94K\x02u\x85\x94R\x94.'
~~~

​		唯一的问题是不能改变unpickle_game_state函数所在模块的路径。一旦用一个函数序列化了数据，它必须在导入路径上保持可用，以便将来反序列化。

**要点**

* pickle内置模块仅用于在受信任的程序之间序列化和反序列化对象
* 如果所涉及的类随着时间的推移发生了变化(例如，添加或删除了属性)，那么先前pickle的对象的反序列化可能会中断
* 使用带有pickle的copyreg内置模块来确保序列化对象的向后兼容性

## 第69条 在需要准确计算的场合，用decimal表示相应的数值

​		Python是编写与数字数据交互的代码的优秀语言。Python的整数类型可以表示任何实际大小的值。其双精度浮点型符合IEEE 754标准。该语言还为虚数提供了标准复数类型。然而，这并不能满足所有情况。

​		例如，假设想计算向一个客户收取国际电话费用的金额。我知道客户在电话上的分秒时间(比如3分42秒)。还有一个从美国打电话到南极洲的固定费率(每分钟1.45美元)。费用是什么?

​		对于浮点数学，计算的费用似乎是合理的

~~~python
rate = 1.45
seconds = 3*60 + 42
cost = rate * seconds / 60
print(cost)
>>>
5.364999999999999
~~~

​		由于IEEE 754浮点数的表示方式，结果比正确的值(5.365)少了0.0001。可能想把这个值四舍五入到5.37，以适当地覆盖客户产生的所有成本。然而，由于浮点错误，舍入到最接近的整数美分实际上减少了最终费用(从5.364到5.36)，而不是增加它(从5.365到5.37):

~~~python
print(round(cost, 2))
>>>
5.36
~~~

​		解决方案是使用Decimal内置模块中的Decimal类。Decimal类默认提供28位小数的定点数学。如果需要的话，它还可以更高。这可以解决IEEE 754浮点数中的精度问题。这个类还提供了对舍入行为的更多控制

​		例如，用Decimal重做南极洲计算，结果是精确的预期电荷而不是近似值:

~~~python
from decimal import Decimal
rate = Decimal('1.45')
seconds = Decimal(3*60 + 42)
cost = rate * seconds / Decimal(60)
print(cost)
>>>
5.365
~~~

​		十进制实例可以以两种不同的方式给出初始值。第一种方法是将包含数字的str传递给Decimal构造函数。这确保了不会由于Python浮点数的固有特性而导致精度损失。第二种方法是直接将float或int实例传递给构造函数。在这里，可以看到两种施工方法导致不同的行为。

~~~python
print(Decimal('1.45'))
print(Decimal(1.45))
>>>
1.45
1.4499999999999999555910790149937383830547332763671875
~~~

​		如果向Decimal构造函数提供整数，就不会出现同样的问题:

~~~python
print('456')
print(456)
>>>
456
456
~~~

​		如果关心确切的答案，请谨慎行事，并使用Decimal类型的str构造函数。

​		回到电话通话的例子，我也想支持非常短的电话之间的地方连接便宜得多(如托莱多和底特律)。在这里，计算了一个5秒长的电话的费用，费率是每分钟0.05美元:

~~~python
rate = Decimal('0.05')
seconds = Decimal('5')
small_cost = rate * seconds / Decimal(60)
print(small_cost)
>>>
0.004166666666666666666666666667
~~~

​		结果是如此之低，以至于当试图将它四舍五入到最接近的整数时，它会下降到零。这是不行的!

~~~python
print(round(small_cost, 2))
>>>
0.00
~~~

​		幸运的是，Decimal类有一个内置函数，用于舍入到所需的小数位。这适用于早期的高成本案例:

~~~python
from decimal import ROUND_UP
rounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)
print(f'Rounded {cost} to {rounded}')
>>>
Rounded 5.365 to 5.37
~~~

​		使用这种量化方法还可以正确地处理简短、廉价的电话的小使用情况:。

~~~python
rounded = small_cost.quantize(Decimal('0.01'),
rounding=ROUND_UP)
print(f'Rounded {small_cost} to {rounded}')
>>>
Rounded 0.004166666666666666666666666667 to 0.01
~~~

​		虽然Decimal对于定点数非常有效，但它在精度上仍然有限制(例如，1/3将是一个近似值)。为了表示有理数而不受精度限制，可以考虑使用fractions内置模块中的Fraction类。

**要点**

* Python在模块中有内置的类型和类，它们实际上可以表示数值的每一种类型
* Decimal类非常适合需要高精度和控制舍入行为的情况，比如货币值的计算
* 如果需要计算精确的答案而不是浮点近似值，那么将str实例传递给Decimal构造函数而不是浮点实例

## 第70条 先分析性能，然后再优化

​		Python的动态特性导致其运行时性能出现令人惊讶的行为。可能认为很慢的操作实际上非常快(例如，字符串操作、生成器)。可能认为很快的语言特性实际上很慢(例如，属性访问，函数调用)。Python程序变慢的真正原因可能不太清楚。

​		最好的方法是忽略直觉，在尝试优化程序之前直接测量程序的性能。Python提供了一个内置的分析器来确定程序的哪些部分负责它的执行时间。这意味着可以将优化工作集中在最大的麻烦来源上，而忽略程序中不影响速度的部分(即，遵循Amdahl定律)。	

​		例如，假设想确定为什么程序中的一个算法很慢。这里，定义了一个函数，使用插入排序对数据列表进行排序:

```python
def insertion_sort(data):
    result = []
    for value in data:
        insert_value(result, value)
    return result
```

​		插入排序的核心机制是为每个数据片段找到插入点的函数。这里，定义了insert_value函数的一个极其低效的版本，它对输入数组进行线性扫描：

```python
def insert_value(array, value):
    for i, existing in enumerate(array):
        if existing > value:
            array.insert(i, value)
            return
    array.append(value)
```

​		为了分析insertion_sort和insert_value，创建了一个随机数数据集，并定义了一个传递给分析器的测试函数:

```python
from random import randint

max_size = 10**4
data = [randint(0, max_size) for _ in range(max_size)]
test = lambda: insertion_sort(data)
```

​		Python提供了两个内置的分析器:一个是纯Python (profile)，另一个是C扩展模块(cProfile)。cProfile内置模块更好，因为它在被分析时对程序性能的影响最小。纯python的替代方案会带来很高的开销，从而导致结果的倾斜。

***

*Note*

​		在分析Python程序时，请确保度量的是代码本身，而不是外部系统。注意访问网络或磁盘资源的功能。由于底层系统的缓慢，这些可能对程序的执行时间有很大的影响。如果程序使用缓存来掩盖这些慢资源的延迟，那么应该确保在开始分析之前适当地预热它。

***

​		这里，从cProfile模块实例化了一个Profile对象，并使用runcall方法通过它运行测试函数:

```python
from cProfile import Profile

profiler = Profile()
profiler.runcall(test)
```

​		当测试函数完成运行后，可以通过使用pstats内置模块及其Stats类提取有关其性能的统计信息。Stats对象上的各种方法调整了如何选择和排序分析信息，以只显示关注的东西:

```python
from pstats import Stats

stats = Stats(profiler)
stats = Stats(profiler, stream=STDOUT)
stats.strip_dirs()
stats.sort_stats('cumulative')
stats.print_stats()
```

​		输出是一个按函数组织的信息表。数据样本只在上面的runcall方法中，剖析器处于活动状态时获取:

~~~python
>>>
20003 function calls in 1.406 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    1.406    1.406 test.py:19(<lambda>)
        1    0.003    0.003    1.406    1.406 test.py:1(insertion_sort)
    10000    1.392    0.000    1.403    0.000 test.py:7(insert_value)
     9988    0.011    0.000    0.011    0.000 {method 'insert' of 'list' objects}
       12    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
~~~

​		下面是一个关于分析器统计数据列含义的快速指南:

* ncalls:分析期间对函数的调用次数
* tottime:执行函数所花费的秒数，不包括执行它调用的其他函数所花费的时间。
* tottime percall:每次调用函数所花费的平均秒数，不包括执行它调用的其他函数所花费的时间。这是总时间除以ncalls
* cumtime:执行函数所花费的累计秒数，包括调用它的所有其他函数所花费的时间
* cumtime percall:每次调用函数所花费的平均秒数，包括它调用的所有其他函数所花费的时间。这是cumtime除以ncalls

​	 查看上面的分析器统计表，可以看到在测试中CPU的最大使用量是在insert_value函数中花费的累积时间。在这里，我重新定义了该函数，以使用bisect内置模块(参见第72条):

```python
from bisect import bisect_left

def insert_value(array, value):
    i = bisect_left(array, value)
    array.insert(i, value)
```

​		可以再次运行分析器并生成一个分析器统计信息的新表。新函数要快得多，所花费的累积时间比之前的insert_value函数少了近100倍:

~~~python
profiler = Profile()
profiler.runcall(test)
stats = Stats(profiler, stream=STDOUT)
stats.strip_dirs()
stats.sort_stats('cumulative')
stats.print_stats()
>>>
         30003 function calls in 0.021 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.021    0.021 item_70.py:71(<lambda>)
        1    0.002    0.002    0.021    0.021 item_70.py:50(insertion_sort)
    10000    0.004    0.000    0.019    0.000 item_70.py:94(insert_value)
    10000    0.011    0.000    0.011    0.000 {method 'insert' of 'list' objects}
    10000    0.005    0.000    0.005    0.000 {built-in method _bisect.bisect_left}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}


         20242 function calls in 0.141 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.141    0.141 item_70.py:122(my_program)
       20    0.004    0.000    0.140    0.007 item_70.py:114(first_func)
    20200    0.137    0.000    0.137    0.000 item_70.py:109(my_utility)
       20    0.000    0.000    0.001    0.000 item_70.py:118(second_func)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}


   Ordered by: cumulative time

Function                                          was called by...
                                                      ncalls  tottime  cumtime
item_70.py:122(my_program)                        <- 
item_70.py:114(first_func)                        <-      20    0.004    0.140  item_70.py:122(my_program)
item_70.py:109(my_utility)                        <-   20000    0.136    0.136  item_70.py:114(first_func)
                                                         200    0.001    0.001  item_70.py:118(second_func)
item_70.py:118(second_func)                       <-      20    0.000    0.001  item_70.py:122(my_program)
{method 'disable' of '_lsprof.Profiler' objects}  <- 
~~~

​		有时在分析整个程序时，可能会发现一个公共实用函数负责大部分执行时间。分析器的默认输出使这种情况难以理解，因为它没有显示工具函数被程序的许多不同部分调用。

​		例如，这里my_utility函数被程序中两个不同的函数重复调用:

```PYTHON
def my_utility(a, b):
    c = 1
    for i in range(100):
        c += a * b

def first_func():
    for _ in range(1000):
        my_utility(4, 5)

def second_func():
    for _ in range(10):
        my_utility(1, 3)

def my_program():
    for _ in range(20):
        first_func()
        second_func()
```

​		分析这段代码并使用默认的print_stats输出会生成令人困惑的统计信息:

~~~python
>>>
         20242 function calls in 0.118 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.118    0.118 item_70.py:122(my_program)
       20    0.003    0.000    0.117    0.006 item_70.py:114(first_func)
    20200    0.115    0.000    0.115    0.000 item_70.py:109(my_utility)
       20    0.000    0.000    0.001    0.000 item_70.py:118(second_func)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}

~~~

​		my_utility函数显然是大多数执行时间的来源，但它为什么被频繁调用并不明显。如果搜索程序的代码，您将发现my_utility的多个调用站点，但仍然感到困惑。

​		为了处理这个问题，Python分析器提供了print_callers方法来显示哪个调用者提供了每个函数的分析信息:

~~~python
stats.print_callers()
~~~

​		这个分析器统计表在左边显示了调用的函数，在右边显示了哪个函数负责进行调用。在这里，显然my_utility是被first_func使用最多的:

~~~python
   Ordered by: cumulative time

Function                                          was called by...
                                                      ncalls  tottime  cumtime
item_70.py:122(my_program)                        <- 
item_70.py:114(first_func)                        <-      20    0.003    0.117  item_70.py:122(my_program)
item_70.py:109(my_utility)                        <-   20000    0.114    0.114  item_70.py:114(first_func)
                                                         200    0.001    0.001  item_70.py:118(second_func)
item_70.py:118(second_func)                       <-      20    0.000    0.001  item_70.py:122(my_program)
{method 'disable' of '_lsprof.Profiler' objects}  <- 

~~~

**要点**

* 在优化之前对Python程序进行分析是很重要的，因为慢的原因通常是模糊的
* 使用cProfile模块而不是profile模块，因为它提供了更准确的概要信息
* Profile对象的runcall方法提供了孤立地分析函数调用树所需的一切
* Stats对象允许您选择并打印需要查看的概要信息子集，以了解程序的性能

## 第71条 优先考虑用deque实现生产者-消费者队列

​		编写程序的一个常见需求是先进先出(FIFO)队列，也称为生产者-消费者队列。当一个函数收集要处理的值，而另一个函数按照接收它们的顺序处理它们时，就使用FIFO队列。通常，程序员使用Python的内置列表类型作为FIFO队列。

​		例如，假设有一个程序，它正在处理传入的电子邮件进行长期归档，并且它正在为生产者-消费者队列使用一个列表。这里，定义了一个类来表示消息:

```python
class Email:
    def __init__(self, sender, receiver, message):
        self.sender = sender
        self.receiver = receiver
        self.message = message
def get_emails():
    yield Email('foo@example.com', 'bar@example.com', 'hello1')
    yield Email('baz@example.com', 'banana@example.com', 'hello2')
    yield None
    yield Email('meep@example.com', 'butter@example.com', 'hello3')
    yield Email('stuff@example.com', 'avocado@example.com', 'hello4')
    yield None
    yield Email('thingy@example.com', 'orange@example.com', 'hello5')
    yield Email('roger@example.com', 'bob@example.com', 'hello6')
    yield None
    yield Email('peanut@example.com', 'alice@example.com', 'hello7')
    yield None

EMAIL_IT = get_emails()
```

​		还定义了一个占位符函数，用于接收来自套接字、文件系统或其他类型的I/O系统的单个电子邮件。这个函数的实现并不重要;重要的是它的接口:它将返回一个Email实例或引发一个NoEmailError异常:

```python
class NoEmailError(Exception):
    pass

def try_receive_email():
    # Returns an Email instance or raises NoEmailError
    try:
        email = next(EMAIL_IT)
    except StopIteration:
        email = None

    if not email:
        raise NoEmailError

    print(f'Produced email: {email.message}')
    return email
```

​		生成函数接收电子邮件并将它们排队以待以后使用。该函数使用列表中的append方法将新消息添加到队列的末尾，以便在之前接收到的所有消息之后处理它们:

```python
def produce_emails(queue):
    while True:
        try:
            email = try_receive_email()
        except NoEmailError:
            return
        else:
            queue.append(email)  # Producer
```

​		消费函数对电子邮件做了一些有用的事情。这个函数调用队列上的pop(0)，它从列表中删除第一个项并将其返回给调用者。通过始终从队列的开始处理项目，消费者确保项目按照它们接收到的顺序进行处理:

```python
def consume_one_email(queue):
    if not queue:
        return
    email = queue.pop(0)  # Consumer
    # Index the message for long-term archival
    print(f'Consumed email: {email.message}')
```

​		最后，需要一个将各个部分连接在一起的循环函数。这个函数在产生和消耗之间交替，直到keep_running函数返回False(参见第60条):

```python
def loop(queue, keep_running):
    while keep_running():
        produce_emails(queue)
        consume_one_email(queue)

def make_test_end():
    count=list(range(10))

    def func():
        if count:
            count.pop()
            return True
        return False

    return func


def my_end_func():
    pass

my_end_func = make_test_end()
loop([], my_end_func)
```

​		为什么不处理由try_receive_email返回的produce_email中的每个Email消息?这归结于延迟和吞吐量之间的权衡。当使用生产者-消费者队列时，通常希望最小化接受新项目的延迟，以便能够尽可能快地收集新项目。然后使用者可以以一致的速度处理积压的条目——在本例中，每个循环一个条目——这以端到端延迟为代价提供了稳定的性能profile和一致的吞吐量(参见第55条)。

​		对于这样的生产者-消费者队列使用列表在一定程度上是可行的，但是随着基数(列表中的项目数量)的增加，列表类型的性能可能会超线性地下降。为了分析使用list作为FIFO队列的性能，可以使用timeit内置模块运行一些微基准测试。在这里，定义了一个使用list的append方法(匹配producer函数的用法)向队列中添加新项的性能基准:

```python
import timeit

def print_results(count, tests):
    avg_iteration = sum(tests) / len(tests)
    print(f'Count {count:>5,} takes {avg_iteration:.6f}s')
    return count, avg_iteration

def list_append_benchmark(count):
    def run(queue):
        for i in range(count):
            queue.append(i)

    tests = timeit.repeat(
        setup='queue = []',
        stmt='run(queue)',
        globals=locals(),
        repeat=1000,
        number=1)

    return print_results(count, tests)
```

​		使用不同级别的基数运行这个基准函数可以比较它与数据大小的关系:

```python
def print_delta(before, after):
    before_count, before_time = before
    after_count, after_time = after
    growth = 1 + (after_count - before_count) / before_count
    slowdown = 1 + (after_time - before_time) / before_time
    print(f'{growth:>4.1f}x data size, {slowdown:>4.1f}x time')

baseline = list_append_benchmark(500)
for count in (1_000, 2_000, 3_000, 4_000, 5_000):
    print()
    comparison = list_append_benchmark(count)
    print_delta(baseline, comparison)
>>>
Count 500 takes 0.000039s
Count 1,000 takes 0.000073s
2.0x data size, 1.9x time
Count 2,000 takes 0.000121s
4.0x data size, 3.1x time
Count 3,000 takes 0.000172s
6.0x data size, 4.5x time
Count 4,000 takes 0.000240s
8.0x data size, 6.2x time
Count 5,000 takes 0.000304s
10.0x data size, 7.9x time
```

​		这表明，对于列表类型，append方法所花费的时间大致是常数，而排队的总时间随着数据大小的增加呈线性增长。当添加新项时，列表类型会在底层增加其容量，但开销相当低，并且会在重复调用中摊平。

​		在这里，为pop(0)调用定义了一个类似的基准测试，该调用从队列的开头删除项(与consumer函数的用法相匹配):

```python
def list_pop_benchmark(count):
    def prepare():
        return list(range(count))

    def run(queue):
        while queue:
            queue.pop(0)

    tests = timeit.repeat(
        setup='queue = prepare()',
        stmt='run(queue)',
        globals=locals(),
        repeat=1000,
        number=1)

    return print_results(count, tests)
```

​		类似地，可以为不同大小的队列运行这个基准测试，看看基数如何影响性能:

```python
baseline = list_pop_benchmark(500)
for count in (1_000, 2_000, 3_000, 4_000, 5_000):
    print()
    comparison = list_pop_benchmark(count)
    print_delta(baseline, comparison)
>>>
Count 500 takes 0.000050s
Count 1,000 takes 0.000133s
2.0x data size, 2.7x time
Count 2,000 takes 0.000347s
4.0x data size, 6.9x time
Count 3,000 takes 0.000663s
6.0x data size, 13.2x time
Count 4,000 takes 0.000943s
8.0x data size, 18.8x time
Count 5,000 takes 0.001481s
10.0x data size, 29.5x time
```

​		令人惊讶的是，这表明从pop(0)列表中退出队列项目的总时间随着队列长度的增加呈二次增长。原因是pop(0)需要将列表中的每一项移回索引，从而有效地重新分配整个列表的内容。需要为列表中的每一项调用pop(0)，因此最终执行大致的len(queue) * len(queue)操作来消耗队列。这并不是规模。

​		Python提供了collection内置模块中的deque类来解决这个问题。Deque是一个双端队列实现。它提供常量时间操作，用于从开始或结束插入或删除项。这使得它非常适合FIFO队列。

​		要使用deque类，对produce_emails中的append调用可以保持与使用队列列表时相同。列表中。consume_one_email中的Pop方法调用必须更改为调用deque。不带参数的Popleft方法。循环方法必须使用deque实例而不是列表来调用。其他的都保持不变。在这里，重新定义了一个受影响的函数，使用新的方法并再次运行循环:

```python
import collections

def consume_one_email(queue):
    if not queue:
        return
    email = queue.popleft()  # Consumer
    # Process the email message
    print(f'Consumed email: {email.message}')

def my_end_func():
    pass

my_end_func = make_test_end()
EMAIL_IT = get_emails()
loop(collections.deque(), my_end_func)
```

​		可以运行另一个版本的基准测试，以验证append性能(匹配producer函数的使用)大致保持不变(取一个常数因子的模):

```python
def deque_append_benchmark(count):
    def prepare():
        return collections.deque()

    def run(queue):
        for i in range(count):
            queue.append(i)

    tests = timeit.repeat(
        setup='queue = prepare()',
        stmt='run(queue)',
        globals=locals(),
        repeat=1000,
        number=1)
    return print_results(count, tests)

baseline = deque_append_benchmark(500)
for count in (1_000, 2_000, 3_000, 4_000, 5_000):
    print()
    comparison = deque_append_benchmark(count)
    print_delta(baseline, comparison)
>>>
Count 500 takes 0.000029s
Count 1,000 takes 0.000059s
2.0x data size, 2.1x time
Count 2,000 takes 0.000121s
4.0x data size, 4.2x time
Count 3,000 takes 0.000171s
6.0x data size, 6.0x time
Count 4,000 takes 0.000243s
8.0x data size, 8.5x time
Count 5,000 takes 0.000295s
10.0x data size, 10.3x time
```

​		还可以对调用popleft的性能进行基准测试，以模拟消费者函数对deque的使用:

```python
def dequeue_popleft_benchmark(count):
    def prepare():
        return collections.deque(range(count))

    def run(queue):
        while queue:
            queue.popleft()

    tests = timeit.repeat(
        setup='queue = prepare()',
        stmt='run(queue)',
        globals=locals(),
        repeat=1000,
        number=1)

    return print_results(count, tests)

baseline = dequeue_popleft_benchmark(500)
for count in (1_000, 2_000, 3_000, 4_000, 5_000):
    print()
    comparison = dequeue_popleft_benchmark(count)
    print_delta(baseline, comparison)
>>>
Count 500 takes 0.000024s
Count 1,000 takes 0.000050s
2.0x data size, 2.1x time
Count 2,000 takes 0.000100s
4.0x data size, 4.2x time
Count 3,000 takes 0.000152s
6.0x data size, 6.3x time
Count 4,000 takes 0.000207s
8.0x data size, 8.6x time
Count 5,000 takes 0.000265s
10.0x data size, 11.0x time
```

​		poppleft的使用是线性的，而不是之前测量的pop(0)的超线性行为。如果知道程序的性能严重依赖于生产者-消费者队列的速度，那么deque是一个很好的选择。如果不确定，那么应该测试程序来找出(参见第70条)。

**要点**

* 通过让生产者调用追加以添加项目，让消费者调用pop(0)来接收项目，列表类型可以用作FIFO队列。但是，这可能会导致问题，因为pop(0)的性能会随着队列长度的增加而呈超线性下降
* collections内置模块中的deque类对于append和popleft占用常量时间(无论长度如何)，这使它成为FIFO队列的理想选择。

## 第72条 考虑用bisect搜索已排序的序列

  	通常会发现内存中有大量数据作为排序列表，然后希望进行搜索。例如，可能已经加载了一个用于拼写检查的英文字典，或者一个日期为日期的财务交易列表，以便进行正确性审计。

​		不管程序需要处理什么数据，当调用index方法时，在列表中搜索一个特定的值需要的时间与列表的长度成正比:

~~~python
data = list(range(10**5))
index = data.index(91234)
print(index)
~~~

​		如果不确定正在搜索的确切值是否在列表中，那么可能希望搜索与目标值相等或超过目标值的最接近的索引。最简单的方法是线性扫描列表，并将每个项目与目标价值进行比较:

```python
def find_closest(sequence, goal):
    for index, value in enumerate(sequence):
        if goal < value:
            return index
    raise ValueError(f'{goal} is out of bounds')

index = find_closest(data, 91234.56)
print(index)
>>>
91235
```

​		Python内置的bisect模块提供了更好的方法来通过有序列表完成这些类型的搜索。可以使用bisect_left函数对任何排序项序列进行有效的二分搜索。它返回的索引要么是条目已经出现在列表中的位置，要么是你想插入条目以使其保持有序的位置::

```python
from bisect import bisect_left

index = bisect_left(data, 91234)     # Exact match
print(index)

index = bisect_left(data, 91234.56)  # Closest match
print(index)
>>>
91234
91235
```

​		二分搜索算法所使用的二分搜索算法的复杂度是对数的。这意味着使用bisect搜索长度为100万的列表所花费的时间与使用该列表线性搜索长度为20的列表所花费的时间大致相同。索引方法(math.log2(10**6) == 19.93…)。这是更快!

​		可以通过使用timeit内置模块运行一个微基准测试来验证上述示例的速度改进:

```python
import random
import timeit

size = 10**5
iterations = 1000

data = list(range(size))
to_lookup = [random.randint(0, size)
             for _ in range(iterations)]

def run_linear(data, to_lookup):
    for index in to_lookup:
        data.index(index)

def run_bisect(data, to_lookup):
    for index in to_lookup:
        bisect_left(data, index)

baseline = timeit.timeit(
    stmt='run_linear(data, to_lookup)',
    globals=globals(),
    number=10)
print(f'Linear search takes {baseline:.6f}s')

comparison = timeit.timeit(
    stmt='run_bisect(data, to_lookup)',
    globals=globals(),
    number=10)
print(f'Bisect search takes {comparison:.6f}s')

slowdown = 1 + ((baseline - comparison) / comparison)
print(f'{slowdown:.1f}x time')
>>>
Linear search takes 6.412933s
Bisect search takes 0.006496s
987.3x time
```

​		关于bisect最好的部分是它不局限于列表类型;你可以将它用于任何类似序列的Python对象(参见第43条)。该模块还为更高级的情况提供了额外的特性(参见help(bisect))。

**要点**

* 使用index方法或简单比较的for循环搜索列表中包含的排序数据需要线性时间
* bisect内置模块的bisect_left函数花费对数时间在排序列表中搜索值，比其他方法快几个数量级

